---
title: "Variational autoencoders"
description: |
  A deep dive into variational autoencoders.
date: "2025-09-28"
categories: [Machine Learning]
---

# Introduction

This document dives into [variational autoencoders (VAE)](https://arxiv.org/abs/1312.6114).

## Why variational autoencoders?

The first question we can ask is why create variational autoencoders?

- Variational autoencoders are generative models that allow to generate data such as text, images or sounds for instance.

- Unlike autoencoders, variational autoencoders allow interpolation in their latent space.

  - This is because autoencoders produce a fixed encoding for a given input while variational autoencoders produce a probability distribution.

  - This forces the decoder of a variational autoencoder to learn to decode over continuous regions of the latent space, not only from specific single points as in an autoencoder.

  - We can find more information about this in this [blog post](https://medium.com/data-science/intuitively-understanding-variational-autoencoders-1bfe67eb5daf).

# Formalization

## Problem setting

- We have a dataset of samples of a random variable $x$.

- We assume it is generated by a random process $p$ and a random variable $z$.

## Goal

The goal is to perform Bayesian inference, i.e. estimate $p(z|x)$.

This will allow to build meaningful representations $z$ of $x$.

These representations can then be used for example to:

- Cluster samples from $x$ based on their representation $z$.

- Edit samples from $x$ by manipulating their representation $z$.

- Generate samples $x$ by sampling from the distribution of $z$ which can be chosen to be simple to sample from.

## Problem

One approach could be to

- Choose a simple distribution $p(z)$.

- Use the Bayes' rule $p(z|x) = \frac{p(z,\ x)}{p(x)} = \frac{p(x|z)p(z)}{\int_{z}^{}p(x|z)p(z)dz}$ to compute $p(z|x)$.

- However, $p(x) = \int_{z}^{}p(x|z)p(z)dz$ is usually intractable because:

  - $p(x|z)$ is unknown.

  - We could parameterize it with $p_{\theta}(x|z)$.

  - But even for moderately complicated likelihood functions $p_{\theta}(x|z)$ such as a neural network with a nonlinear hidden layer for example, there is no closed-form solution to the integral $p_{\theta}(x) = \int_{z}^{}p_{\theta}(x|z)p(z)dz$.

  - Also, $z$ usually is in high dimension:

    - Integrating over such a high-dimensional space is computationally impossible.

    - Monte Carlo estimations are too expensive.

- Thus, we cannot compute $p(x)$ which prevents us from computing $p(z|x)$.

## Strategy

Model $p(z|x)$ by a parametric distribution $q_{\phi}(z|x)$ and minimize $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ (see [Kullback--Leibler divergence](/posts/2025-09-11-information-theory/index.qmd#kullback--leibler-divergence)).

$q_{\phi}(z|x)$ is sometimes called the *variational distribution* (see [Glossary](#glossary)).

Minimize the reversed $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ and not the forward $KL(p(z|x)\ ||\ q_{\phi}(z|x))$ because $p(z|x)$ is unknown thus, the expectation under $p(z|x)$ cannot be computed.

This results in mode seeking behavior. See:

- [Forward vs reversed Kullback-Leibler divergence](/posts/2025-09-11-information-theory/index.qmd#forward-vs-reversed-kullback-leibler-divergence)

- and [Consequences of minimizing the reversed Kullback-Leibler divergence between the posteriors](#consequences-of-minimizing-the-reversed-kullback-leibler-divergence-between-the-posteriors)

# Find tractable objective

## Steps

Here is a summary of the different steps to estimate $q_{\phi}(z|x)$. The details for each step are given below.

- The goal is

Minimize $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ with respect to $\phi$ (Objective 1)

- $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ still contains the intractable $p(z|x)$ we are looking for but it can be expressed differently.

- Rearrange $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ as$KL(q_{\phi}(z|x)\ ||\ p(z|x)) = KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$\
  (see [details below](#rearrange-klq_phizx-pzx-asklq_phizx-pzx-klq_phizx-pz---e_q_phizxlbrack-log-pxzrbrack-log-px)).

- This gives the [equivalent objective](#equivalent-objective)

Minimize $KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack$ with respect to $\phi$ (Objective 2)

- However, $p(x|z)$ is unknown.

- We can parameterize it with $p_{\theta}(x|z)$ and learn it from data.

- Focus on the tractable terms and define a new objective (see [details below](#focus-on-the-tractable-terms-and-define-a-new-objective)).

Maximize$E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z)) = log\ p_{\theta}(x) - KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$\
with respect to $\phi$ and $\theta$ (Objective 3)

- (Objective 3) is not equivalent to (Objective 1) and (Objective 2), it is an approximation.

- (Objective 3) $E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z))$ is called the ELBO (see [Origin of the ELBO name](#origin-of-the-elbo-name)).

## Summary

- The initial objective was

Minimize $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ with respect to $\phi$ (Objective 1)

- It is intractable thus, an approximate objective is used instead

Maximize $ELBO = E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z))$ with respect to $\phi$ and $\theta$ (Objective 3)

## Rearrange $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ as$KL(q_{\phi}(z|x)\ ||\ p(z|x)) = KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$

- We ideally want to minimize $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ with respect to $\phi$.

- But as seen above, the true posterior $p(z|x)$ is unknown and intractable.

- Using the Bayes' rule, $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ can be rewritten in terms of

  - $p(x|z)$

  - $p(x)$

  - $p(z)$

- As seen above $p(x)$ is intractable but

  - $p(z)$ can be chosen to be a simple distribution.

  - $p(x|z)$ is unknown but it could be latter parameterized with $p_{\theta}(x|z)$.

- The idea is to rewrite $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ in terms of $p(x|z)$, $p(x)$ and $p(z)$ and to focus on the tractable terms.

- $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ can be rewritten as

$KL(q_{\phi}(z|x)\ ||\ p(z|x)) = KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$

$KL(q_{\phi}(z|x)\ ||\ p(z|x))$

$= \ E_{q_{\phi}(z|x)}\lbrack log\ \frac{q_{\phi}(z|x)}{p(z|x)}\rbrack$

$= \ E_{q_{\phi}(z|x)}\lbrack log\ \frac{q_{\phi}(z|x)\ p(x)}{p(x|z)p(z)}\rbrack$ (by Bayes' rule $p(z|x)p(x) = p(x|z)p(z)$)

$= \ E_{q_{\phi}(z|x)}\lbrack log\ \frac{q_{\phi}(z|x)}{p(z)} - log\ p(x|z) + log\ p(x)\rbrack$

$= \ E_{q_{\phi}(z|x)}\lbrack log\ \frac{q_{\phi}(z|x)}{p(z)}\rbrack - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$

$= KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$

## Equivalent objective

$KL(q_{\phi}(z|x)\ ||\ p(z|x)) = KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack + log\ p(x)$

- $p(x)$ does not depend on $\phi$, thus minimizing $KL(q_{\phi}(z|x)\ ||\ p(z|x))$ with respect to $\phi$ is equivalent to maximizing $KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack$.

## Focus on the tractable terms and define a new objective

- $p(x|z)$ is unknown.

- We can parameterize it with $p_{\theta}(x|z)$ and learn it from data.

- We now have two sets of parameters $\phi$ and $\theta$ to learn.

- Our new objective is to minimize

$KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x)) = KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack + log\ p_{\theta}(x)$

with respect to $\phi$ and $\theta$.

- Notice that the equivalence mentioned above is only true for fixed parameters $\theta$.

  - When $\phi$ and $\theta$ are learned jointly, $log\ p_{\theta}(x)$ is no longer constant.

  - When the values of $\theta$ change, the target for $q_{\phi}(z|x)$ in $KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$ changes too.

  - Thus, maximizing $KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$ with respect to $\phi$ and $\theta$ is not equivalent to minimizing $KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$ with respect to $\phi$ and $\theta$.

- We focus anyway on maximizing $KL(q_{\phi}(z|x)\ ||\ p(z)) - E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$ because these are the only tractable terms.

- We can rearrange the expression above as

$E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z)) = log\ p_{\theta}(x) - KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$

# Interpretation of the tractable objective

The new objective is

Maximize (Objective 3)

$ELBO$\
$= log\ p_{\theta}(x) - KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$ (1)

$= E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z))$ (2)

with respect to $\phi$ and $\theta$

We can see two types of terms in the expression above

- $q_{\phi}$: the encoder which maps the observed data $x$ to its representation $z$.

- $p_{\theta}$: the decoder which reconstruct $x$ from representations $z$.

Maximizing the expression above can be interpreted as

- Term (1)

  - The decoder $p_{\theta}$ is trained to maximize the marginal log likelihood of the data $x$.

  - The encoder $q_{\phi}$ is trained to approximate the posterior $p_{\theta}(z|x)\  \propto \ p_{\theta}(x|z)\ p(z)$ implied by the decoder $p_{\theta}(x|z)$ and the prior $p(z)$.

- Term (2)

  - The decoder $p_{\theta}$ is trained to maximize the log likelihood of the data $x$ given its representation $z$ built by the encoder $q_{\phi}$.

  - The encoder $q_{\phi}$ is trained to produce representations $z$ close to the prior $p(z)$ and consistent with the likelihood of the decoder $p_{\theta}(x|z)$.

  - $KL(q_{\phi}(z|x)\ ||\ p(z))$

    - It can be seen as a regularization term that forces the estimated posterior $q_{\phi}(z|x)$ to be close to the chosen prior $p(z)$.

    - This regularization is applied to each data sample $x$. Thus, it encourages the posterior $q_{\phi}(z|x)$ to be the same for all data samples $x$, i.e. it encourages the posterior $q_{\phi}(z|x)$ to not convey any information unique to the individual data sample $x$.

    - In this way, the $KL$ term and the likelihood term are conflicting.

The $ELBO$ enforces a mutual consistency:

- The encoder reflects the latent structure suggested by the decoder,

- And the decoder adapts to the latent codes produced by the encoder, all while fitting the observed data.

# Maximize $ELBO = E_{q_{\phi}(z|x)}\lbrack log\ \ p_{\theta}(x|z)\rbrack - \ KL(q_{\phi}(z|x)\ ||\ \ p_{\theta}(z))$

Now that the objective has been defined, this section focuses on how to address it. Again, the summary gives an overview of the different steps and the details for each step are given below.

## Summary

The objective is to maximize the $ELBO = E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z))$.

- [Gradients with respect to](#gradient-with-respect-to-theta) $\theta$ (for a single sample $x$)

  - $\nabla_{\theta}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\theta}log\ p_{\theta}(x|z_{i})\rbrack$ with $z_{i} \sim q_{\phi}(z|x)$

- [Gradient with respect to](#gradient-with-respect-to-phi)$\ \phi$ (for a single sample $x$)

  - [Expectation part](#expectation-part)

  $\nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i})))$ with $\varepsilon_{i} \sim q(\varepsilon)$

- Kullback-Leibler divergence part

  $p(z)$ and $q_{\phi}(z|x)$ can be chosen in a way that allows the Kullback-Leibler divergence part to be analytically derived (see [Kullback-Leibler divergence part](#derive-the-gradient-of-the-kullback-leibler-divergence-part) in the [Practical application](#practical-application) section).

- $\nabla_{\phi}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i}))) - \nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z))$ with $\varepsilon_{i} \sim q(\varepsilon)$

## Gradient with respect to $\theta$

$\nabla_{\theta}ELBO = \nabla_{\theta}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$

$q_{\phi}(z|x)$ does not depend on $\theta$ thus (see [Expectation and derivative](/posts/2025-09-28-expectation-and-derivative/index.qmd)):

$\nabla_{\theta}ELBO = E_{q_{\phi}(z|x)}\lbrack\nabla_{\theta}log\ p_{\theta}(x|z)\rbrack$

Which can be estimated with a Monte Carlo estimation of the expectation with the mean:\
$\nabla_{\theta}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\theta}log\ p_{\theta}(x|z_{i})\rbrack$ with $z_{i} \sim q_{\phi}(z|x)$

## Gradient with respect to $\phi$

$\nabla_{\phi}ELBO = \nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - \nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z))$

The gradient above contains two terms. Let us treat them separately.

### Expectation part

The expectation depends on $\phi$, thus, the gradient cannot be written as $E_{q_{\phi}(z|x)}\lbrack\nabla_{\phi}log\ p_{\theta}(x|z)\rbrack$.\
As explained in [Expectation and derivative](/posts/2025-09-28-expectation-and-derivative/index.qmd), there are several solutions. Two of them are:

- Use the [score function estimator](/posts/2025-09-28-expectation-and-derivative/index.qmd#score-function-estimator)

- Or the [reparametrization trick](/posts/2025-09-28-expectation-and-derivative/index.qmd#reparameterization-trick)

The reparameterization is used because is offers a lower gradient variance than the score function estimator and because $p_{\theta}(x|z)$ is differentiable.

#### Application of the reparameterization trick

$\nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$

We can

- Express our random variable $z$

  - $z \sim q_{\phi}(z|x)$

- As a function $g_{\phi}$ of another random variable $\varepsilon$

  - $z = g_{\phi}(\varepsilon)$ with $\varepsilon \sim n(\varepsilon)$

Then the [Law of the Unconscious Statistician (LOTUS)](/posts/2025-09-28-expectation-and-derivative/index.qmd#law-of-the-unconscious-statistician-lotus) tells us that

- The expectation of $z$ can be computed with the distribution of $\varepsilon$

  - $\nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$

  $= \nabla_{\phi}E_{n(\varepsilon)}\lbrack log\ p_{\theta}(x|g_{\phi}(\varepsilon))\rbrack$

With this new formulation, the distribution no longer depends on $\phi$. Thus we are in a [Fixed distribution](/posts/2025-09-28-expectation-and-derivative/index.qmd#fixed-distribution) setting.

Thus,

$\nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$

$= \nabla_{\phi}E_{n(\varepsilon)}\lbrack log\ p_{\theta}(x|g_{\phi}(\varepsilon))\rbrack$ ([LOTUS](/posts/2025-09-28-expectation-and-derivative/index.qmd#law-of-the-unconscious-statistician-lotus))

${= E}_{n(\varepsilon)}\lbrack\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon)))\rbrack$ ($n$ does not depend on $\phi$)

which can be estimated with Monte Carlo

$\nabla_{\phi}E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack$

$\simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i})))$ with $\varepsilon_{i} \sim q(\varepsilon)$

### Kullback-Leibler divergence part

$p(z)$ and $q_{\phi}(z|x)$ can be chosen in a way that allows the Kullback-Leibler divergence part to be analytically derived (see [Kullback-Leibler divergence part](#derive-the-gradient-of-the-kullback-leibler-divergence-part) in the [Practical application](#practical-application) section).

# Origin of the $ELBO$ name

- $ELBO = log\ p_{\theta}(x) - KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$.

- Thus, $ELBO \leq log\ p_{\theta}(x)$ because the $KL$ term is always positive.

- $log\ p_{\theta}(x)$ is the $\log$ of the evidence of the data.

- Hence the name *Evidence Lower Bound*.

# Practical application

## Derive the gradient of the Kullback-Leibler divergence part

### Summary

As shown in the [Details](#details) section below, the Kullback-Leibler divergence term in the $ELBO$ can be derived analytically for well chosen $p(z)$ and $q_{\phi}(z|x)$ distributions.

More specifically,

- For $p(z) = N(z;0,\ I)$ and

  $\nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z)) = \nabla_{\phi}\frac{1}{2}\sum_{i = 1}^{k}(\mu_{i}^{2} + \sigma_{i}^{2} - log\ \sigma_{i}^{2} - 1)$

And thus,

- $\nabla_{\phi}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i}))) + \nabla_{\phi}\frac{1}{2}\sum_{i = 1}^{k}(\mu_{i}^{2} + \sigma_{i}^{2} - log\ \sigma_{i}^{2} - 1)$ with $\varepsilon_{i} \sim q(\varepsilon)$

### Details

The Kullback-Leibler divergence term is $\nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z))$.

Let us choose both distributions $q_{\phi}(z|x)$ and $p(z)$ to be multivariate gaussian distributions of dimension $k$.

- $p(z) = N(z;0,\ I)$

- $q_{\phi}(z|x) = N(z;\mu,\Sigma)$

- The probability density distribution for a multivariate gaussian is

$N(x;\mu,\Sigma) = {(2\pi)}^{- k/2}{det(\Sigma)}^{- 1/2}e^{- \frac{1}{2}{(x - \mu)}^{T}\Sigma^{- 1}(x - \mu)}$

$log\ N(x;\mu,\Sigma) = - \frac{1}{2}\lbrack{(x - \mu)}^{T}\Sigma^{- 1}(x - \mu) + k\ log(2\pi) + log\ det(\Sigma))$

- Thus, for $p(z)$ and $q_{\phi}(z|x)$, we have:

$\log{\ q}_{\phi}(z|x) = - \frac{1}{2}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu) + k\ log(2\pi) + log\ det(\Sigma))$

$log\ p(z) = - \frac{1}{2}\lbrack z^{T}z + k\ log(2\pi)\rbrack$

- Which gives

$KL(q_{\phi}(z|x)\ ||\ p(z))$

$= E_{q_{\phi}(z|x)}\lbrack log\ q_{\phi}(z|x) - log\ p(z)\rbrack$

$= - \frac{1}{2}E_{q_{\phi}(z|x)}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu) + k\ log(2\pi) + log\ det(\Sigma)) - z^{T}z - k\ log(2\pi)\rbrack$

$= \frac{1}{2}E_{q_{\phi}(z|x)}\lbrack z^{T}z\  - {(z - \mu)}^{T}\Sigma^{- 1}(z - \mu)\rbrack - \frac{1}{2}log\ det(\Sigma))$

- Let us first consider $E_{q_{\phi}(z|x)}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu)\rbrack$

$E_{q_{\phi}(z|x)}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu)\rbrack$

$= tr(E_{q_{\phi}(z|x)}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu)\rbrack)$ (because $E_{q_{\phi}(z|x)}\lbrack{(z - \mu)}^{T}\Sigma^{- 1}(z - \mu)\rbrack$ is a scalar)

$= tr(\Sigma^{- 1}E_{q_{\phi}(z|x)}\lbrack(z - \mu){(z - \mu)}^{T}\rbrack)$ (by the [cyclic property of the trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Cyclic_property))

$= tr(\Sigma^{- 1}\Sigma\rbrack)$

$= tr(I)$

$= k$

- Let us now consider $E_{q_{\phi}(z|x)}\lbrack z^{T}z\rbrack$

$E_{q_{\phi}(z|x)}\lbrack z^{T}z\rbrack$

$= E_{q_{\phi}(z|x)}\lbrack{tr(z}^{T}z)\rbrack)$

$= tr(E_{q_{\phi}(z|x)}\lbrack zz^{T}\rbrack)$

If we write $z = \mu + (z - \mu)$

${tr(E}_{q_{\phi}(z|x)}\lbrack zz^{T}\rbrack)$

$= tr(E_{q_{\phi}(z|x)}\lbrack(\mu + (z - \mu)){(\mu + (z - \mu))}^{T}\rbrack)$

$= tr(E_{q_{\phi}(z|x)}\lbrack\mu\mu^{T} + (z - \mu){(z - \mu)}^{T} + \mu{(z - \mu)}^{T} + (z - \mu)\mu\rbrack)$

$= tr(E_{q_{\phi}(z|x)}\lbrack\mu\mu^{T} + (z - \mu){(z - \mu)}^{T}\rbrack)$ (because $E_{q_{\phi}(z|x)}\lbrack z - \mu\rbrack = 0$)

$= tr(\mu\mu^{T} + \Sigma)$

$= \mu\mu^{T} + tr(\Sigma)$

- Finally, we have

$KL(q_{\phi}(z|x)\ ||\ p(z)) = \frac{1}{2}(\mu\mu^{T} + tr(\Sigma) - k - log\ det(\Sigma))$

- If we choose $\Sigma = diag(\sigma_{1}^{2},\ \sigma_{2}^{2},\ ...,\ \sigma_{k}^{2})$

$KL(q_{\phi}(z|x)\ ||\ p(z))$

$= \frac{1}{2}(\mu\mu^{T} + tr(\Sigma) - k - log\ det(\Sigma))$

$= \frac{1}{2}(\mu\mu^{T} + \sum_{i = 1}^{k}\sigma_{i}^{2} - k - \sum_{i = 1}^{k}log\ \sigma_{i}^{2})$

$=$$\frac{1}{2}\sum_{i = 1}^{k}(\mu_{i}^{2} + \sigma_{i}^{2} - log\ \sigma_{i}^{2} - 1)$

## Define the loss function

As seen above, the final objective is to maximize the $ELBO$ which has the following gradients with respect to $\theta$ and $\phi$.

- $\nabla_{\theta}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\theta}log\ p_{\theta}(x|z_{i})\rbrack$ with $z_{i} \sim q_{\phi}(z|x)$

- $\nabla_{\phi}ELBO \simeq \frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i}))) - \nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z))$ with $\varepsilon_{i} \sim q(\varepsilon)$

The $\nabla_{\phi}KL(q_{\phi}(z|x)\ ||\ p(z))$ is addressed in the [Derive the gradient of the Kullback-Leibler divergence part](#derive-the-gradient-of-the-kullback-leibler-divergence-part) section above.

The remaining two terms $\frac{1}{N}\sum_{i = 1}^{N}\nabla_{\theta}log\ p_{\theta}(x|z_{i})\rbrack$ and $\frac{1}{N}\sum_{i = 1}^{N}\nabla_{\phi}(log\ p_{\theta}(x|g_{\phi}(\varepsilon_{i})))$ both involve the log likelihood of the data $x$.

As explained in [Maximum Likelihood Estimation and loss functions](/posts/2025-09-11-likelihood-loss/index.qmd), maximizing the likelihood (Maximum Likelihood Estimation or MLE) of the observed data is equivalent to minimizing:

- The cross entropy loss in a supervised classification setting.

- The mean squared error (MSE) in a regression setting and under the assumption of normally distributed errors.

Thus, depending of the choice of $\ p_{\theta}(x|z)$, the cross entropy loss (binary cross entropy if there are only 2 classes) or the mean squared error (MSE) can be chosen (but other losses are also possible).

Here is a summary of different possible applications.

+------------+----------------+---------------+---------------+---------------------+
| Data type  | Problem type   | Decoder       | Loss function | Example of          |
| for $x$    |                | outputs       | corresponding | application         |
|            |                |               | to MLE        |                     |
+============+================+===============+===============+=====================+
| Continuous | Regression     | Predicted     | MSE           | Images (pixels      |
| data       |                | value         |               | intensities)        |
|            |                |               |               |                     |
|            |                |               |               | Audio signal        |
+------------+----------------+---------------+---------------+---------------------+
| Binary     | Binary         | Probability   | Binary cross  | Binary images       |
| data       | classification |               | entropy       |                     |
+------------+----------------+---------------+---------------+---------------------+
| Discrete   | Classification | Class         | Cross entropy | Text tokens         |
| data       |                | probabilities |               |                     |
+------------+----------------+---------------+---------------+---------------------+

## Consequences of minimizing the reversed Kullback-Leibler divergence between the posteriors

As explained in [Strategy](#strategy), the original objective is to minimize the reversed Kullback-Leibler divergence between the posteriors $KL(q_{\phi}(z|x)\ ||\ p(z|x))$.

This results in mode seeking behavior (see [Forward vs reversed Kullback-Leibler divergence](/posts/2025-09-11-information-theory/index.qmd#forward-vs-reversed-kullback-leibler-divergence) to understand why) and has the following consequences:

- Mode collapse

  - If the true posterior $p(z|x)$ is multi-modal, the approximate posterior $q_{\phi}(z|x)$ will tend to pick one mode and ignore the others.

  - This can cause loss of diversity in the learned latent space.

- Overly confident uncertainty estimates

  - The approximate posterior $q_{\phi}(z|x)$ puts all its mass at the same mode.

  - This makes its certainty large at the chosen mode and thus, overconfident.

# Conclusion

## Initial intractable objective

Minimize $KL(q_{\phi}(z|x)\ ||\ p(z|x))$

with respect to $\phi$ (Objective 1)

- Approximate the true posterior $p(z|x)$ with $q_{\phi}(z|x)$.

## Approximate tractable objective

$ELBO = E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z)) = log\ p_{\theta}(x) - KL(q_{\phi}(z|x)\ ||\ p_{\theta}(z|x))$

with respect to $\phi$ and $\theta$ (Objective 3)

Interpreted as

- Maximize evidence $p_{\theta}(x)$ while making posteriors $p_{\theta}(z|x)$ and $q_{\phi}(z|x)$ similar.

- Maximize likelihood $p_{\theta}(x|z)$ while making posterior $q_{\phi}(z|x)$ close to the chosen prior $p(z)$.

- Minimize reconstruction error regularized by $KL$ divergence from chosen prior.

## Maximize $ELBO$ with the reparameterization trick

$ELBO = E_{q_{\phi}(z|x)}\lbrack log\ p_{\theta}(x|z)\rbrack - KL(q_{\phi}(z|x)\ ||\ p(z))$

The gradient with respect to $\phi$ of $E_{q_{\phi}(z|x)}$ term cannot be simply estimated with Monte Carlo estimation. The reparameterization trick is used instead.

# Glossary

## Variational distribution

$q_{\phi}(z|x)$

## Evidence Lower Bound (ELBO) or variational lower bound

${ELBO = E}_{q_{\phi}(z|x)}\lbrack log\ p(x,\ z) - log\ q_{\phi}(z|x)\rbrack$

$ELBO = log\ p(x) - \ KL(q_{\phi}(z|x)\ ||\ p(z|x))$

$ELBO \leq log\ p(x) = log\ evidence$

$ELBO = E_{q_{\phi}(z|x)}\lbrack log\ p(x|z)\rbrack - \ KL(q_{\phi}(z|x)\ ||\ p(z))$

## Reparameterization gradients

a.k.a pathwise gradients.

The gradients obtained with the reparameterization trick.

# Ressources

## Variational autoencoders

[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)

[https://mbernste.github.io/posts/vae/](https://mbernste.github.io/posts/vae/)

## Autoencoders vs variational autoencoders

[https://medium.com/data-science/intuitively-understanding-variational-autoencoders-1bfe67eb5daf](https://medium.com/data-science/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)

## ELBO

### Definition

[https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html](https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html)

### Gradient

[https://mpatacchiola.github.io/blog/2021/02/08/intro-variational-inference-2.html](https://mpatacchiola.github.io/blog/2021/02/08/intro-variational-inference-2.html)

## Kullback Leibler divergence

[https://kvfrans.com/deriving-the-kl/](https://kvfrans.com/deriving-the-kl/)

## Reparameterization trick

[https://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html](https://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html)

[https://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/](https://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/)
