[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Derivatives, chain rule and backpropagation\n\n\n\nCalculus\n\n\n\nThis post is a brief summary of how derivatives and chain rule are defined for scalar functions and vector-valued functions with univariate or multivariate inputs.\n\n\n\n\n\nAug 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA test blog post\n\n\n\nCategory 1\n\nCategory 2\n\n\n\nPost description\n\n\n\n\n\nAug 9, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nisace’s website",
    "section": "",
    "text": "Welcome the Nisace’s website."
  },
  {
    "objectID": "posts/2025-08-09-test/index.html",
    "href": "posts/2025-08-09-test/index.html",
    "title": "A test blog post",
    "section": "",
    "text": "Text…"
  },
  {
    "objectID": "posts/2025-08-09-test/index.html#heading-1",
    "href": "posts/2025-08-09-test/index.html#heading-1",
    "title": "A test blog post",
    "section": "",
    "text": "Text…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a univariate scalar function, the derivative is simply defined as the derivative of the function.\n\n\\(\\frac{df}{d\\theta} \\in R\\)\n\n\n\\(f:\\ R \\rightarrow R\\), \\(\\theta \\in R,\\ f(\\theta) \\in R\\)\n\n\n\n\n\nFor a multivariate scalar function, the partial derivatives of \\(f:\\ R^{n} \\rightarrow R\\) can be arranged in its gradient:\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\).\nThe gradient is a column vector where each element is the partial derivative of \\(f\\) with respect to the corresponding input dimension.\n\n\n\\(f:\\ R^{n} \\rightarrow R\\), \\(\\theta \\in R^{n},\\ f(\\theta) \\in R\\)\n\n\n\n\n\nThe partial derivatives of \\(f:\\ R^{n} \\rightarrow R^{m}\\)can be arranged in its Jacobian matrix \\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\).\n\\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\) contains\n\nIn its rows, \\({\\nabla_{\\theta}}^{T}f_{i} \\in R^{n}\\) the transpose of the gradient of the components \\(f_{i}\\) of \\(f\\).\n\nEach row \\(i\\) contains the derivative of the component \\(f_{i}\\) with respect to all the input dimensions.\n\nIn its columns, \\(\\frac{\\partial f}{\\partial\\theta_{j}}\\) the partial derivatives of \\(f\\) with respect to \\(\\theta_{j}\\).\n\nEach column \\(j\\) contains the derivation of all the components of \\(f\\) with respect to \\(\\theta_{j}\\) (the input dimension \\(j\\)).\n\n\n\\(\\frac{\\partial f}{\\partial\\theta}^{T} = \\left\\lbrack \\nabla_{\\theta}f_{1},\\ \\nabla_{\\theta}f_{2},\\ ...,\\ \\nabla_{\\theta}f_{m} \\right\\rbrack \\in R^{n\\  \\times \\ m}\\)\n\n\n\\(f\\) is a multivariate scalar function and its Jacobian is the transpose of its gradient.\n\n\n\n\\(f\\) is an univariate scalar function.\n\n\n\n\\(\\theta \\in R^{n},\\ f(\\theta) \\in R^{m}\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a univariate scalar function, the derivative is simply defined as the derivative of the function.\n\n\\(\\frac{df}{d\\theta} \\in R\\)\n\n\n\\(f:\\ R \\rightarrow R\\), \\(\\theta \\in R,\\ f(\\theta) \\in R\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a multivariate scalar function, the partial derivatives of \\(f:\\ R^{n} \\rightarrow R\\) can be arranged in its gradient:\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\).\nThe gradient is a column vector where each element is the partial derivative of \\(f\\) with respect to the corresponding input dimension.\n\n\n\\(f:\\ R^{n} \\rightarrow R\\), \\(\\theta \\in R^{n},\\ f(\\theta) \\in R\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-vector-valued-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-vector-valued-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "The partial derivatives of \\(f:\\ R^{n} \\rightarrow R^{m}\\)can be arranged in its Jacobian matrix \\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\).\n\\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\) contains\n\nIn its rows, \\({\\nabla_{\\theta}}^{T}f_{i} \\in R^{n}\\) the transpose of the gradient of the components \\(f_{i}\\) of \\(f\\).\n\nEach row \\(i\\) contains the derivative of the component \\(f_{i}\\) with respect to all the input dimensions.\n\nIn its columns, \\(\\frac{\\partial f}{\\partial\\theta_{j}}\\) the partial derivatives of \\(f\\) with respect to \\(\\theta_{j}\\).\n\nEach column \\(j\\) contains the derivation of all the components of \\(f\\) with respect to \\(\\theta_{j}\\) (the input dimension \\(j\\)).\n\n\n\\(\\frac{\\partial f}{\\partial\\theta}^{T} = \\left\\lbrack \\nabla_{\\theta}f_{1},\\ \\nabla_{\\theta}f_{2},\\ ...,\\ \\nabla_{\\theta}f_{m} \\right\\rbrack \\in R^{n\\  \\times \\ m}\\)\n\n\n\\(f\\) is a multivariate scalar function and its Jacobian is the transpose of its gradient.\n\n\n\n\\(f\\) is an univariate scalar function.\n\n\n\n\\(\\theta \\in R^{n},\\ f(\\theta) \\in R^{m}\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function-1",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function-1",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Univariate scalar function",
    "text": "Univariate scalar function\nWhen univariate scalar functions are composed, the derivative is the product of the intermediate derivatives.\n\n\\(\\frac{df}{d\\theta} = \\frac{df}{dz}\\frac{dz}{d\\theta}\\)\n\nNotes\n\\(\\theta \\in R,\\ z = g(\\theta) \\in R\\), \\(g:\\ R \\rightarrow R\\), \\(f:\\ R \\rightarrow R\\), \\(z = g(\\theta)\\), \\(f = g(z) \\in R\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function-1",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function-1",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Multivariate scalar function",
    "text": "Multivariate scalar function\n\n\n\\(f:\\ R^{n} \\rightarrow R\\) is a multivariate scalar function. Its partial derivatives can be arranged in its gradient.\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\)\nFor each input dimension \\(\\theta_{i}\\), the chain rule is applied to each intermediate dimension \\(z_{j}\\) and the derivative is the sum over \\(j\\).\nThus,\n\\(\\frac{\\partial f}{\\partial\\theta_{i}} = \\sum_{j = 1}^{m}\\frac{\\partial f}{\\partial z_{j}}\\frac{\\partial z_{j}}{\\partial\\theta_{i}}\\)\nThe expression of \\({\\nabla_{\\theta}}^{T}f\\) above can also be expressed with:\n\nThe Jacobian of \\(z\\) with respect to \\(\\theta\\): \\(\\frac{\\partial z}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\)\nThe gradient of \\(f\\) with respect to \\(z\\): \\(\\nabla_{z}f \\in R^{m}\\)\n\n\\(\\nabla_{\\theta}f = \\left( \\frac{\\partial z}{\\partial\\theta} \\right)^{T}\\nabla_{z}f\\)\n\nNotes\n\\(\\theta \\in R^{n},\\ z = g(\\theta) \\in R^{m}\\), \\(z = g(\\theta)\\), \\(g:\\ R^{n} \\rightarrow R^{m}\\), \\(f:\\ R^{m} \\rightarrow R\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#generic-form",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#generic-form",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Generic form",
    "text": "Generic form\nThe sections below explain how to compute the derivatives of \\(E\\) with respect to:\n\n\\(a^{l}\\), the activation of layer \\(l\\)\n\\(z^{l}\\), the value of layer \\(l\\) before the activation\n\\(w_{ij}^{l}\\), the weight between neuron \\(i\\) of layer \\(l - 1\\) and neuron \\(j\\) of layer \\(l\\)\n\n\nDerivative with respect to \\(a^{l}\\)\nTo compute the derivative of \\(E\\) with respect to \\(a^{l}\\), the activation of layer \\(l\\), we express \\(E\\) as the composition of \\(E(z^{l + 1})\\) and \\(z^{l + 1}(a^{l},W^{l + 1})\\):\n\\(E(\\widehat{y},\\ y) = E(...(z^{l + 1}(a^{l},W^{l + 1})))\\)\n\\(E(a^{l})\\) is thus a multivariate scalar function with intermediate variables \\(z^{l + 1}\\).\n\nAccording to the chain rule for a multivariate scalar function, its derivative is given by its gradient:\n\\(\\nabla_{a^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial a_{i}^{l}},\\ ... \\right\\rbrack\\)\nwhere \\(\\frac{\\partial E}{\\partial a_{i}^{l}} = \\frac{\\partial E}{\\partial z^{l + 1}}\\frac{\\partial z^{l + 1}}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}\\frac{\\partial z_{k}^{l + 1}}{\\partial a_{i}^{l}}\\) Multivariate chain rule\nMoreover, \\(z_{j}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ij}^{l}a_{i}^{l - 1}\\), thus \\(\\frac{\\partial z_{j}^{l}}{\\partial a_{i}^{l - 1}} = w_{ij}^{l}\\) and the expression above becomes:\n\\(\\frac{\\partial E}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}{\\ w}_{ik}^{l + 1}\\)\n\n\n\nDerivative with respect to \\(z^{l}\\)\nSimilarly, we can express \\(E\\) as the composition of \\(E(a^{l})\\) and \\(a^{l}(z^{l})\\):\n\\(E(\\widehat{y},\\ y) = E(...z^{l + 1}(a^{l}(z^{l}),W^{l + 1})))\\)\n\\(E(z^{l})\\) is thus a multivariate scalar function and its derivative is given by its gradient:\n\\(\\nabla_{z^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial z_{i}^{l}},\\ ... \\right\\rbrack\\)\nNotice also that each function \\(E(z_{i}^{l})\\) can be expressed as the composition of \\(E(a_{i}^{l})\\) and \\(a_{i}^{l}(z_{i}^{l})\\).\nThus, the chain rule for univariate scalar functions can be applied:\n\\(\\frac{\\partial E}{\\partial z_{i}^{l}} = \\frac{\\partial E}{\\partial a_{i}^{l}}\\frac{\\partial a_{i}^{l}}{\\partial z_{i}^{l}}\\) Scalar chain rule\n\n\n\nDerivative with respect to \\(w_{ij}^{l}\\)\nFinally we can express \\(E\\) as the composition of \\(E(z^{l})\\) and \\(z^{l}(a^{l - 1},W^{l})\\):\n\\(E(\\widehat{y},\\ y) = E(...z^{l}(a^{l - 1},W^{l})))\\)\n\\(E(W^{l})\\) is thus a multivariate scalar function with intermediate variables \\(z^{l}\\).\n\nAccording to the chain rule for a multivariate scalar function, its derivative is given by its gradient:\n\\(\\nabla_{W^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial w_{ij}^{l}},\\ ... \\right\\rbrack\\)\nwhere \\(\\frac{\\partial E}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial w_{ij}^{l}} = \\sum_{k = 1}^{N^{l}}\\frac{\\partial E}{\\partial z_{k}^{l}}\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w_{ij}^{l}}\\) Multivariate chain rule\n\nMoreover, \\(z_{k}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ik}^{l}a_{i}^{l - 1}\\)\nThus \\(\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = 0\\) for \\(k \\neq j\\)\nAnd the expression above becomes:\n\\(\\frac{\\partial E}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w_{ij}^{l}}\\)\n\n\n\nLayer \\(L\\)\n\nDerivative wrt \\(a^{L}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L},y)\\)\n\\(E\\) is a multivariate real value function \\(E:\\ R^{N^{L}} \\rightarrow R\\)\n\\(N^{l}\\): number of neurons in layer \\(l\\)\nThe derivative of \\(E\\ wrt\\ a^{L}\\) is thus a gradient of dimension \\(R^{N^{L}}\\)\n\\(\\nabla_{a^{L}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial a_{I}^{L}},\\frac{\\partial E}{\\partial a_{2}^{L}},\\ ...,\\frac{\\partial E}{\\partial a_{i}^{L}},\\ ... \\right\\rbrack\\) Gradient of a multivariate function\n\n\nDerivative wrt \\(z^{L}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L}(z^{L}),y)\\)\n\\(\\nabla_{z^{L}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial z_{I}^{L}},\\frac{\\partial E}{\\partial z_{2}^{L}},\\ ...,\\frac{\\partial E}{\\partial z_{i}^{L}},\\ ... \\right\\rbrack\\) Gradient of a multivariate function\n\\(a_{i}^{L} = a_{i}^{L}(z_{i}^{L})\\) Scalar function\n\\(E(\\widehat{y},\\ y) = E(a_{i}^{L}(z_{i}^{L}),y)\\)\n\\(\\frac{\\partial E}{\\partial z_{i}^{L}} = \\frac{\\partial E}{\\partial a_{i}^{L}}\\frac{\\partial a_{i}^{L}}{\\partial z_{i}^{L}}\\) Scalar chain rule\n\n\nDerivative wrt \\(w_{ij}^{L}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L}(z^{L}(a^{L - 1},W^{L},b^{L})))\\)\n\\(\\nabla_{W^{L}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial w_{ij}^{L}},\\ ... \\right\\rbrack\\) Gradient of a multivariate function\n\\(\\frac{\\partial E}{\\partial w_{ij}^{L}} = \\sum_{k = 1}^{N^{L}}\\frac{\\partial E}{\\partial z_{k}^{L}}\\frac{\\partial z_{k}^{L}}{\\partial w_{ij}^{L}} = \\frac{\\partial E}{\\partial z_{j}^{L}}\\frac{\\partial z_{j}^{L}}{\\partial w_{ij}^{L}}\\) Multivariate chain rule\nbecause \\(\\frac{\\partial z_{k}^{L}}{\\partial w_{ij}^{L}} = 0\\) for \\(k \\neq j\\)\n\n\n\nLayer \\(L - 1\\)\n\nDerivative wrt \\(a^{L - 1}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L}(z^{L}(a^{L - 1},W^{L},b^{L})))\\)\n\\(\\nabla_{a^{L - 1}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial a_{i}^{L - 1}},\\ ... \\right\\rbrack\\)\n\\(\\frac{\\partial E}{\\partial a_{i}^{L - 1}} = \\sum_{k = 1}^{N^{L}}\\frac{\\partial E}{\\partial z_{k}^{L}}\\frac{\\partial z_{k}^{L}}{\\partial a_{i}^{L - 1}} = \\sum_{k = 1}^{N^{L}}\\frac{\\partial E}{\\partial z_{k}^{L}}{\\ w}_{ik}^{L}\\) Multivariate chain rule\n\\(z_{j}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ij}^{l}a_{i}^{l - 1}\\)\n\\(\\frac{\\partial z_{j}^{l}}{\\partial a_{i}^{l - 1}} = w_{ij}^{l}\\)\n\n\nDerivative wrt \\(z^{L - 1}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L}(z^{L}(a^{L - 1}(z^{L - 1}),W^{L},b^{L})))\\)\n\\(\\nabla_{z^{L - 1}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial z_{i}^{L - 1}},\\ ... \\right\\rbrack\\)\n\\(\\frac{\\partial E}{\\partial z_{i}^{L - 1}} = \\frac{\\partial E}{\\partial a_{i}^{L - 1}}\\frac{\\partial a_{i}^{L - 1}}{\\partial z_{i}^{L - 1}}\\) Scalar chain rule\n\n\nDerivative wrt \\(w_{ij}^{L - 1}\\)\n\\(E(\\widehat{y},\\ y) = E(a^{L}(z^{L}(a^{L - 1}(z^{L - 1}(a^{L - 2},W^{L - 1},b^{L - 1})),W^{L},b^{L})))\\)\n\\(\\nabla_{W^{L - 1}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial w_{ij}^{L - 1}},\\ ... \\right\\rbrack\\) Gradient of a multivariate function\n\\(\\frac{\\partial E}{\\partial w_{ij}^{L - 1}} = \\sum_{k = 1}^{N^{L - 1}}\\frac{\\partial E}{\\partial z_{k}^{L - 1}}\\frac{\\partial z_{k}^{L - 1}}{\\partial w_{ij}^{L - 1}} = \\frac{\\partial E}{\\partial z_{j}^{L - 1}}\\frac{\\partial z_{j}^{L - 1}}{\\partial w_{ij}^{L - 1}}\\) Multivariate chain rule\nbecause \\(\\frac{\\partial z_{k}^{L - 1}}{\\partial w_{ij}^{L - 1}} = 0\\) for \\(k \\neq j\\)\n\n\n\nLayer \\(l\\)\n\nDerivative wrt \\(a^{l}\\)\n\\(E(\\widehat{y},\\ y) = E(...(z^{l + 1}(a^{l},W^{l + 1},b^{l + 1})))\\)\n\\(\\nabla_{a^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial a_{i}^{l}},\\ ... \\right\\rbrack\\)\n\\(\\frac{\\partial E}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}\\frac{\\partial z_{k}^{l + 1}}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}{\\ w}_{ik}^{l + 1}\\) Multivariate chain rule\n\\(z_{j}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ij}^{l}a_{i}^{l - 1}\\)\n\\(\\frac{\\partial z_{j}^{l}}{\\partial a_{i}^{l - 1}} = w_{ij}^{l}\\)\n\n\nDerivative wrt \\(z^{l}\\)\n\\(E(\\widehat{y},\\ y) = E(...z^{l + 1}(a^{l}(z^{l}),W^{l},b^{l})))\\)\n\\(\\nabla_{z^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial z_{i}^{l}},\\ ... \\right\\rbrack\\)\n\\(\\frac{\\partial E}{\\partial z_{i}^{l}} = \\frac{\\partial E}{\\partial a_{i}^{l}}\\frac{\\partial a_{i}^{l}}{\\partial z_{i}^{l}}\\) Scalar chain rule\n\n\nDerivative wrt \\(w_{ij}^{l}\\)\n\\(E(\\widehat{y},\\ y) = E(...z^{l}(a^{l - 1},W^{l},b^{l})))\\)\n\\(\\nabla_{W^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial w_{ij}^{l}},\\ ... \\right\\rbrack\\) Gradient of a multivariate function\n\\(\\frac{\\partial E}{\\partial w_{ij}^{l}} = \\sum_{k = 1}^{N^{l}}\\frac{\\partial E}{\\partial z_{k}^{l}}\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w_{ij}^{l}}\\) Multivariate chain rule\nbecause \\(\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = 0\\) for \\(k \\neq j\\)"
  }
]