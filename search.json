[
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html",
    "href": "posts/2025-09-28-variational-autoencoders/index.html",
    "title": "Variational autoencoders",
    "section": "",
    "text": "This document dives into variational autoencoders (VAE).\n\n\nThe first question we can ask is why create variational autoencoders?\n\nVariational autoencoders are generative models that allow to generate data such as text, images or sounds for instance.\nUnlike autoencoders, variational autoencoders allow interpolation in their latent space.\n\nThis is because autoencoders produce a fixed encoding for a given input while variational autoencoders produce a probability distribution.\nThis forces the decoder of a variational autoencoder to learn to decode over continuous regions of the latent space, not only from specific single points as in an autoencoder.\nWe can find more information about this in this blog post."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#why-variational-autoencoders",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#why-variational-autoencoders",
    "title": "Variational autoencoders",
    "section": "",
    "text": "The first question we can ask is why create variational autoencoders?\n\nVariational autoencoders are generative models that allow to generate data such as text, images or sounds for instance.\nUnlike autoencoders, variational autoencoders allow interpolation in their latent space.\n\nThis is because autoencoders produce a fixed encoding for a given input while variational autoencoders produce a probability distribution.\nThis forces the decoder of a variational autoencoder to learn to decode over continuous regions of the latent space, not only from specific single points as in an autoencoder.\nWe can find more information about this in this blog post."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#problem-setting",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#problem-setting",
    "title": "Variational autoencoders",
    "section": "Problem setting",
    "text": "Problem setting\n\nWe have a dataset of samples of a random variable \\(x\\).\nWe assume it is generated by a random process \\(p\\) and a random variable \\(z\\)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#goal",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#goal",
    "title": "Variational autoencoders",
    "section": "Goal",
    "text": "Goal\nThe goal is to perform Bayesian inference, i.e. estimate \\(p(z|x)\\).\nThis will allow to build meaningful representations \\(z\\) of \\(x\\).\nThese representations can then be used for example to:\n\nCluster samples from \\(x\\) based on their representation \\(z\\).\nEdit samples from \\(x\\) by manipulating their representation \\(z\\).\nGenerate samples \\(x\\) by sampling from the distribution of \\(z\\) which can be chosen to be simple to sample from."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#problem",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#problem",
    "title": "Variational autoencoders",
    "section": "Problem",
    "text": "Problem\nOne approach could be to\n\nChoose a simple distribution \\(p(z)\\).\nUse the Bayes’ rule \\(p(z|x) = \\frac{p(z,\\ x)}{p(x)} = \\frac{p(x|z)p(z)}{\\int_{z}^{}p(x|z)p(z)dz}\\) to compute \\(p(z|x)\\).\nHowever, \\(p(x) = \\int_{z}^{}p(x|z)p(z)dz\\) is usually intractable because:\n\n\\(p(x|z)\\) is unknown.\nWe could parameterize it with \\(p_{\\theta}(x|z)\\).\nBut even for moderately complicated likelihood functions \\(p_{\\theta}(x|z)\\) such as a neural network with a nonlinear hidden layer for example, there is no closed-form solution to the integral \\(p_{\\theta}(x) = \\int_{z}^{}p_{\\theta}(x|z)p(z)dz\\).\nAlso, \\(z\\) usually is in high dimension:\n\nIntegrating over such a high-dimensional space is computationally impossible.\nMonte Carlo estimations are too expensive.\n\n\nThus, we cannot compute \\(p(x)\\) which prevents us from computing \\(p(z|x)\\)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#strategy",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#strategy",
    "title": "Variational autoencoders",
    "section": "Strategy",
    "text": "Strategy\nModel \\(p(z|x)\\) by a parametric distribution \\(q_{\\phi}(z|x)\\) and minimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) (see Kullback–Leibler divergence).\n\\(q_{\\phi}(z|x)\\) is sometimes called the variational distribution (see Glossary).\nMinimize the reversed \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) and not the forward \\(KL(p(z|x)\\ ||\\ q_{\\phi}(z|x))\\) because \\(p(z|x)\\) is unknown thus, the expectation under \\(p(z|x)\\) cannot be computed.\nThis results in mode seeking behavior. See:\n\nForward vs reversed Kullback-Leibler divergence\nand Consequences of minimizing the reversed Kullback-Leibler divergence between the posteriors"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#steps",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#steps",
    "title": "Variational autoencoders",
    "section": "Steps",
    "text": "Steps\nHere is a summary of the different steps to estimate \\(q_{\\phi}(z|x)\\). The details for each step are given below.\n\nThe goal is\n\n\n\nMinimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) with respect to \\(\\phi\\)\n\n\n(Objective 1)\n\n\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) still contains the intractable \\(p(z|x)\\) we are looking for but it can be expressed differently.\nRearrange \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) as\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)\n(see details below).\nThis gives the equivalent objective\n\n\n\nMinimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack\\) with respect to \\(\\phi\\)\n\n\n(Objective 2)\n\n\n\nHowever, \\(p(x|z)\\) is unknown.\nWe can parameterize it with \\(p_{\\theta}(x|z)\\) and learn it from data.\nFocus on the tractable terms and define a new objective (see details below).\n\n\n\nMaximize\\(E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z)) = log\\ p_{\\theta}(x) - KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x))\\)\nwith respect to \\(\\phi\\) and \\(\\theta\\)\n\n\n(Objective 3)\n\n\n\n(Objective 3) is not equivalent to (Objective 1) and (Objective 2), it is an approximation.\n(Objective 3) \\(E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\) is called the ELBO (see Origin of the ELBO name)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#summary",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#summary",
    "title": "Variational autoencoders",
    "section": "Summary",
    "text": "Summary\n\nThe initial objective was\n\n\n\nMinimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) with respect to \\(\\phi\\)\n\n\n(Objective 1)\n\n\n\nIt is intractable thus, an approximate objective is used instead\n\nMaximize \\(ELBO = E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\) with respect to \\(\\phi\\) and \\(\\theta\\) (Objective 3)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#rearrange-klq_phizx-pzx-asklq_phizx-pzx-klq_phizx-pz---e_q_phizxlbrack-log-pxzrbrack-log-px",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#rearrange-klq_phizx-pzx-asklq_phizx-pzx-klq_phizx-pz---e_q_phizxlbrack-log-pxzrbrack-log-px",
    "title": "Variational autoencoders",
    "section": "Rearrange \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) as\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)",
    "text": "Rearrange \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) as\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)\n\nWe ideally want to minimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) with respect to \\(\\phi\\).\nBut as seen above, the true posterior \\(p(z|x)\\) is unknown and intractable.\nUsing the Bayes’ rule, \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) can be rewritten in terms of\n\n\\(p(x|z)\\)\n\\(p(x)\\)\n\\(p(z)\\)\n\nAs seen above \\(p(x)\\) is intractable but\n\n\\(p(z)\\) can be chosen to be a simple distribution.\n\\(p(x|z)\\) is unknown but it could be latter parameterized with \\(p_{\\theta}(x|z)\\).\n\nThe idea is to rewrite \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) in terms of \\(p(x|z)\\), \\(p(x)\\) and \\(p(z)\\) and to focus on the tractable terms.\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) can be rewritten as\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\)\n\\(= \\ E_{q_{\\phi}(z|x)}\\lbrack log\\ \\frac{q_{\\phi}(z|x)}{p(z|x)}\\rbrack\\)\n\n\n\\(= \\ E_{q_{\\phi}(z|x)}\\lbrack log\\ \\frac{q_{\\phi}(z|x)\\ p(x)}{p(x|z)p(z)}\\rbrack\\)\n\n\n(by Bayes’ rule \\(p(z|x)p(x) = p(x|z)p(z)\\))\n\n\n\\(= \\ E_{q_{\\phi}(z|x)}\\lbrack log\\ \\frac{q_{\\phi}(z|x)}{p(z)} - log\\ p(x|z) + log\\ p(x)\\rbrack\\)\n\\(= \\ E_{q_{\\phi}(z|x)}\\lbrack log\\ \\frac{q_{\\phi}(z|x)}{p(z)}\\rbrack - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)\n\\(= KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#equivalent-objective",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#equivalent-objective",
    "title": "Variational autoencoders",
    "section": "Equivalent objective",
    "text": "Equivalent objective\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack + log\\ p(x)\\)\n\n\\(p(x)\\) does not depend on \\(\\phi\\), thus minimizing \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\) with respect to \\(\\phi\\) is equivalent to maximizing \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack\\)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#focus-on-the-tractable-terms-and-define-a-new-objective",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#focus-on-the-tractable-terms-and-define-a-new-objective",
    "title": "Variational autoencoders",
    "section": "Focus on the tractable terms and define a new objective",
    "text": "Focus on the tractable terms and define a new objective\n\n\\(p(x|z)\\) is unknown.\nWe can parameterize it with \\(p_{\\theta}(x|z)\\) and learn it from data.\nWe now have two sets of parameters \\(\\phi\\) and \\(\\theta\\) to learn.\nOur new objective is to minimize\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x)) = KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack + log\\ p_{\\theta}(x)\\)\nwith respect to \\(\\phi\\) and \\(\\theta\\).\n\nNotice that the equivalence mentioned above is only true for fixed parameters \\(\\theta\\).\n\nWhen \\(\\phi\\) and \\(\\theta\\) are learned jointly, \\(log\\ p_{\\theta}(x)\\) is no longer constant.\nWhen the values of \\(\\theta\\) change, the target for \\(q_{\\phi}(z|x)\\) in \\(KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x))\\) changes too.\nThus, maximizing \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\) with respect to \\(\\phi\\) and \\(\\theta\\) is not equivalent to minimizing \\(KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x))\\) with respect to \\(\\phi\\) and \\(\\theta\\).\n\nWe focus anyway on maximizing \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z)) - E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\) because these are the only tractable terms.\nWe can rearrange the expression above as\n\n\\(E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z)) = log\\ p_{\\theta}(x) - KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x))\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#summary-1",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#summary-1",
    "title": "Variational autoencoders",
    "section": "Summary",
    "text": "Summary\nThe objective is to maximize the \\(ELBO = E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\).\n\nGradients with respect to \\(\\theta\\) (for a single sample \\(x\\))\n\n\\(\\nabla_{\\theta}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\theta}log\\ p_{\\theta}(x|z_{i})\\rbrack\\) with \\(z_{i} \\sim q_{\\phi}(z|x)\\)\n\nGradient with respect to\\(\\ \\phi\\) (for a single sample \\(x\\))\n\nExpectation part\n\n\\(\\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i})))\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)\nKullback-Leibler divergence part\n\\(p(z)\\) and \\(q_{\\phi}(z|x)\\) can be chosen in a way that allows the Kullback-Leibler divergence part to be analytically derived (see Kullback-Leibler divergence part in the Practical application section).\n\\(\\nabla_{\\phi}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i}))) - \\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#gradient-with-respect-to-theta",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#gradient-with-respect-to-theta",
    "title": "Variational autoencoders",
    "section": "Gradient with respect to \\(\\theta\\)",
    "text": "Gradient with respect to \\(\\theta\\)\n\\(\\nabla_{\\theta}ELBO = \\nabla_{\\theta}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\)\n\\(q_{\\phi}(z|x)\\) does not depend on \\(\\theta\\) thus (see Expectation and derivative):\n\\(\\nabla_{\\theta}ELBO = E_{q_{\\phi}(z|x)}\\lbrack\\nabla_{\\theta}log\\ p_{\\theta}(x|z)\\rbrack\\)\nWhich can be estimated with a Monte Carlo estimation of the expectation with the mean:\n\\(\\nabla_{\\theta}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\theta}log\\ p_{\\theta}(x|z_{i})\\rbrack\\) with \\(z_{i} \\sim q_{\\phi}(z|x)\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#gradient-with-respect-to-phi",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#gradient-with-respect-to-phi",
    "title": "Variational autoencoders",
    "section": "Gradient with respect to \\(\\phi\\)",
    "text": "Gradient with respect to \\(\\phi\\)\n\\(\\nabla_{\\phi}ELBO = \\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - \\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\)\nThe gradient above contains two terms. Let us treat them separately.\n\nExpectation part\nThe expectation depends on \\(\\phi\\), thus, the gradient cannot be written as \\(E_{q_{\\phi}(z|x)}\\lbrack\\nabla_{\\phi}log\\ p_{\\theta}(x|z)\\rbrack\\).\nAs explained in Expectation and derivative, there are several solutions. Two of them are:\n\nUse the score function estimator\nOr the reparametrization trick\n\nThe reparameterization is used because is offers a lower gradient variance than the score function estimator and because \\(p_{\\theta}(x|z)\\) is differentiable.\n\nApplication of the reparameterization trick\n\\(\\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\)\nWe can\n\nExpress our random variable \\(z\\)\n\n\\(z \\sim q_{\\phi}(z|x)\\)\n\nAs a function \\(g_{\\phi}\\) of another random variable \\(\\varepsilon\\)\n\n\\(z = g_{\\phi}(\\varepsilon)\\) with \\(\\varepsilon \\sim n(\\varepsilon)\\)\n\n\nThen the Law of the Unconscious Statistician (LOTUS) tells us that\n\nThe expectation of \\(z\\) can be computed with the distribution of \\(\\varepsilon\\)\n\n\\(\\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\)\n\n\\(= \\nabla_{\\phi}E_{n(\\varepsilon)}\\lbrack log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon))\\rbrack\\)\n\nWith this new formulation, the distribution no longer depends on \\(\\phi\\). Thus we are in a Fixed distribution setting.\nThus,\n\\(\\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\)\n\n\n\\(= \\nabla_{\\phi}E_{n(\\varepsilon)}\\lbrack log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon))\\rbrack\\)\n\n\n(LOTUS)\n\n\n\n\n\\({= E}_{n(\\varepsilon)}\\lbrack\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon)))\\rbrack\\)\n\n\n(\\(n\\) does not depend on \\(\\phi\\))\n\n\nwhich can be estimated with Monte Carlo\n\\(\\nabla_{\\phi}E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack\\)\n\\(\\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i})))\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)\n\n\n\nKullback-Leibler divergence part\n\\(p(z)\\) and \\(q_{\\phi}(z|x)\\) can be chosen in a way that allows the Kullback-Leibler divergence part to be analytically derived (see Kullback-Leibler divergence part in the Practical application section)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#derive-the-gradient-of-the-kullback-leibler-divergence-part",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#derive-the-gradient-of-the-kullback-leibler-divergence-part",
    "title": "Variational autoencoders",
    "section": "Derive the gradient of the Kullback-Leibler divergence part",
    "text": "Derive the gradient of the Kullback-Leibler divergence part\n\nSummary\nAs shown in the Details section below, the Kullback-Leibler divergence term in the \\(ELBO\\) can be derived analytically for well chosen \\(p(z)\\) and \\(q_{\\phi}(z|x)\\) distributions.\nMore specifically,\n\nFor \\(p(z) = N(z;0,\\ I)\\) and\n\\(\\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z)) = \\nabla_{\\phi}\\frac{1}{2}\\sum_{i = 1}^{k}(\\mu_{i}^{2} + \\sigma_{i}^{2} - log\\ \\sigma_{i}^{2} - 1)\\)\n\nAnd thus,\n\n\\(\\nabla_{\\phi}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i}))) + \\nabla_{\\phi}\\frac{1}{2}\\sum_{i = 1}^{k}(\\mu_{i}^{2} + \\sigma_{i}^{2} - log\\ \\sigma_{i}^{2} - 1)\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)\n\n\n\nDetails\nThe Kullback-Leibler divergence term is \\(\\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\).\nLet us choose both distributions \\(q_{\\phi}(z|x)\\) and \\(p(z)\\) to be multivariate gaussian distributions of dimension \\(k\\).\n\n\\(p(z) = N(z;0,\\ I)\\)\n\\(q_{\\phi}(z|x) = N(z;\\mu,\\Sigma)\\)\nThe probability density distribution for a multivariate gaussian is\n\n\\(N(x;\\mu,\\Sigma) = {(2\\pi)}^{- k/2}{det(\\Sigma)}^{- 1/2}e^{- \\frac{1}{2}{(x - \\mu)}^{T}\\Sigma^{- 1}(x - \\mu)}\\)\n\\(log\\ N(x;\\mu,\\Sigma) = - \\frac{1}{2}\\lbrack{(x - \\mu)}^{T}\\Sigma^{- 1}(x - \\mu) + k\\ log(2\\pi) + log\\ det(\\Sigma))\\)\n\nThus, for \\(p(z)\\) and \\(q_{\\phi}(z|x)\\), we have:\n\n\\(\\log{\\ q}_{\\phi}(z|x) = - \\frac{1}{2}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu) + k\\ log(2\\pi) + log\\ det(\\Sigma))\\)\n\\(log\\ p(z) = - \\frac{1}{2}\\lbrack z^{T}z + k\\ log(2\\pi)\\rbrack\\)\n\nWhich gives\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\)\n\\(= E_{q_{\\phi}(z|x)}\\lbrack log\\ q_{\\phi}(z|x) - log\\ p(z)\\rbrack\\)\n\\(= - \\frac{1}{2}E_{q_{\\phi}(z|x)}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu) + k\\ log(2\\pi) + log\\ det(\\Sigma)) - z^{T}z - k\\ log(2\\pi)\\rbrack\\)\n\\(= \\frac{1}{2}E_{q_{\\phi}(z|x)}\\lbrack z^{T}z\\  - {(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu)\\rbrack - \\frac{1}{2}log\\ det(\\Sigma))\\)\n\nLet us first consider \\(E_{q_{\\phi}(z|x)}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu)\\rbrack\\)\n\n\\(E_{q_{\\phi}(z|x)}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu)\\rbrack\\)\n\n\n\\(= tr(E_{q_{\\phi}(z|x)}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu)\\rbrack)\\)\n\n\n(because \\(E_{q_{\\phi}(z|x)}\\lbrack{(z - \\mu)}^{T}\\Sigma^{- 1}(z - \\mu)\\rbrack\\) is a scalar)\n\n\n\n\n\\(= tr(\\Sigma^{- 1}E_{q_{\\phi}(z|x)}\\lbrack(z - \\mu){(z - \\mu)}^{T}\\rbrack)\\)\n\n\n(by the cyclic property of the trace)\n\n\n\\(= tr(\\Sigma^{- 1}\\Sigma\\rbrack)\\)\n\\(= tr(I)\\)\n\\(= k\\)\n\nLet us now consider \\(E_{q_{\\phi}(z|x)}\\lbrack z^{T}z\\rbrack\\)\n\n\\(E_{q_{\\phi}(z|x)}\\lbrack z^{T}z\\rbrack\\)\n\\(= E_{q_{\\phi}(z|x)}\\lbrack{tr(z}^{T}z)\\rbrack)\\)\n\\(= tr(E_{q_{\\phi}(z|x)}\\lbrack zz^{T}\\rbrack)\\)\nIf we write \\(z = \\mu + (z - \\mu)\\)\n\\({tr(E}_{q_{\\phi}(z|x)}\\lbrack zz^{T}\\rbrack)\\)\n\\(= tr(E_{q_{\\phi}(z|x)}\\lbrack(\\mu + (z - \\mu)){(\\mu + (z - \\mu))}^{T}\\rbrack)\\)\n\\(= tr(E_{q_{\\phi}(z|x)}\\lbrack\\mu\\mu^{T} + (z - \\mu){(z - \\mu)}^{T} + \\mu{(z - \\mu)}^{T} + (z - \\mu)\\mu\\rbrack)\\)\n\\(= tr(E_{q_{\\phi}(z|x)}\\lbrack\\mu\\mu^{T} + (z - \\mu){(z - \\mu)}^{T}\\rbrack)\\) (because \\(E_{q_{\\phi}(z|x)}\\lbrack z - \\mu\\rbrack = 0\\))\n\\(= tr(\\mu\\mu^{T} + \\Sigma)\\)\n\\(= \\mu\\mu^{T} + tr(\\Sigma)\\)\n\nFinally, we have\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z)) = \\frac{1}{2}(\\mu\\mu^{T} + tr(\\Sigma) - k - log\\ det(\\Sigma))\\)\n\nIf we choose \\(\\Sigma = diag(\\sigma_{1}^{2},\\ \\sigma_{2}^{2},\\ ...,\\ \\sigma_{k}^{2})\\)\n\n\\(KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\)\n\\(= \\frac{1}{2}(\\mu\\mu^{T} + tr(\\Sigma) - k - log\\ det(\\Sigma))\\)\n\\(= \\frac{1}{2}(\\mu\\mu^{T} + \\sum_{i = 1}^{k}\\sigma_{i}^{2} - k - \\sum_{i = 1}^{k}log\\ \\sigma_{i}^{2})\\)\n\\(=\\)\\(\\frac{1}{2}\\sum_{i = 1}^{k}(\\mu_{i}^{2} + \\sigma_{i}^{2} - log\\ \\sigma_{i}^{2} - 1)\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#define-the-loss-function",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#define-the-loss-function",
    "title": "Variational autoencoders",
    "section": "Define the loss function",
    "text": "Define the loss function\nAs seen above, the final objective is to maximize the \\(ELBO\\) which has the following gradients with respect to \\(\\theta\\) and \\(\\phi\\).\n\n\\(\\nabla_{\\theta}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\theta}log\\ p_{\\theta}(x|z_{i})\\rbrack\\) with \\(z_{i} \\sim q_{\\phi}(z|x)\\)\n\\(\\nabla_{\\phi}ELBO \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i}))) - \\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)\n\nThe \\(\\nabla_{\\phi}KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\) is addressed in the Derive the gradient of the Kullback-Leibler divergence part section above.\nThe remaining two terms \\(\\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\theta}log\\ p_{\\theta}(x|z_{i})\\rbrack\\) and \\(\\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\phi}(log\\ p_{\\theta}(x|g_{\\phi}(\\varepsilon_{i})))\\) both involve the log likelihood of the data \\(x\\).\nAs explained in Maximum Likelihood Estimation and loss functions, maximizing the likelihood (Maximum Likelihood Estimation or MLE) of the observed data is equivalent to minimizing:\n\nThe cross entropy loss in a supervised classification setting.\nThe mean squared error (MSE) in a regression setting and under the assumption of normally distributed errors.\n\nThus, depending of the choice of \\(\\ p_{\\theta}(x|z)\\), the cross entropy loss (binary cross entropy if there are only 2 classes) or the mean squared error (MSE) can be chosen (but other losses are also possible).\nHere is a summary of different possible applications.\n\n\n\n\n\n\n\n\n\n\nData type for \\(x\\)\nProblem type\nDecoder outputs\nLoss function corresponding to MLE\nExample of application\n\n\n\n\nContinuous data\nRegression\nPredicted value\nMSE\nImages (pixels intensities)\nAudio signal\n\n\nBinary data\nBinary classification\nProbability\nBinary cross entropy\nBinary images\n\n\nDiscrete data\nClassification\nClass probabilities\nCross entropy\nText tokens"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#consequences-of-minimizing-the-reversed-kullback-leibler-divergence-between-the-posteriors",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#consequences-of-minimizing-the-reversed-kullback-leibler-divergence-between-the-posteriors",
    "title": "Variational autoencoders",
    "section": "Consequences of minimizing the reversed Kullback-Leibler divergence between the posteriors",
    "text": "Consequences of minimizing the reversed Kullback-Leibler divergence between the posteriors\nAs explained in Strategy, the original objective is to minimize the reversed Kullback-Leibler divergence between the posteriors \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\).\nThis results in mode seeking behavior (see Forward vs reversed Kullback-Leibler divergence to understand why) and has the following consequences:\n\nMode collapse\n\nIf the true posterior \\(p(z|x)\\) is multi-modal, the approximate posterior \\(q_{\\phi}(z|x)\\) will tend to pick one mode and ignore the others.\nThis can cause loss of diversity in the learned latent space.\n\nOverly confident uncertainty estimates\n\nThe approximate posterior \\(q_{\\phi}(z|x)\\) puts all its mass at the same mode.\nThis makes its certainty large at the chosen mode and thus, overconfident."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#initial-intractable-objective",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#initial-intractable-objective",
    "title": "Variational autoencoders",
    "section": "Initial intractable objective",
    "text": "Initial intractable objective\nMinimize \\(KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\)\n\n\nwith respect to \\(\\phi\\)\n\n\n(Objective 1)\n\n\n\nApproximate the true posterior \\(p(z|x)\\) with \\(q_{\\phi}(z|x)\\)."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#approximate-tractable-objective",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#approximate-tractable-objective",
    "title": "Variational autoencoders",
    "section": "Approximate tractable objective",
    "text": "Approximate tractable objective\n\\(ELBO = E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z)) = log\\ p_{\\theta}(x) - KL(q_{\\phi}(z|x)\\ ||\\ p_{\\theta}(z|x))\\)\n\n\nwith respect to \\(\\phi\\) and \\(\\theta\\)\n\n\n(Objective 3)\n\n\nInterpreted as\n\nMaximize evidence \\(p_{\\theta}(x)\\) while making posteriors \\(p_{\\theta}(z|x)\\) and \\(q_{\\phi}(z|x)\\) similar.\nMaximize likelihood \\(p_{\\theta}(x|z)\\) while making posterior \\(q_{\\phi}(z|x)\\) close to the chosen prior \\(p(z)\\).\nMinimize reconstruction error regularized by \\(KL\\) divergence from chosen prior."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#maximize-elbo-with-the-reparameterization-trick",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#maximize-elbo-with-the-reparameterization-trick",
    "title": "Variational autoencoders",
    "section": "Maximize \\(ELBO\\) with the reparameterization trick",
    "text": "Maximize \\(ELBO\\) with the reparameterization trick\n\\(ELBO = E_{q_{\\phi}(z|x)}\\lbrack log\\ p_{\\theta}(x|z)\\rbrack - KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\)\nThe gradient with respect to \\(\\phi\\) of \\(E_{q_{\\phi}(z|x)}\\) term cannot be simply estimated with Monte Carlo estimation. The reparameterization trick is used instead."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#variational-distribution",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#variational-distribution",
    "title": "Variational autoencoders",
    "section": "Variational distribution",
    "text": "Variational distribution\n\\(q_{\\phi}(z|x)\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#evidence-lower-bound-elbo-or-variational-lower-bound",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#evidence-lower-bound-elbo-or-variational-lower-bound",
    "title": "Variational autoencoders",
    "section": "Evidence Lower Bound (ELBO) or variational lower bound",
    "text": "Evidence Lower Bound (ELBO) or variational lower bound\n\\({ELBO = E}_{q_{\\phi}(z|x)}\\lbrack log\\ p(x,\\ z) - log\\ q_{\\phi}(z|x)\\rbrack\\)\n\\(ELBO = log\\ p(x) - \\ KL(q_{\\phi}(z|x)\\ ||\\ p(z|x))\\)\n\\(ELBO \\leq log\\ p(x) = log\\ evidence\\)\n\\(ELBO = E_{q_{\\phi}(z|x)}\\lbrack log\\ p(x|z)\\rbrack - \\ KL(q_{\\phi}(z|x)\\ ||\\ p(z))\\)"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#reparameterization-gradients",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#reparameterization-gradients",
    "title": "Variational autoencoders",
    "section": "Reparameterization gradients",
    "text": "Reparameterization gradients\na.k.a pathwise gradients.\nThe gradients obtained with the reparameterization trick."
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#variational-autoencoders",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#variational-autoencoders",
    "title": "Variational autoencoders",
    "section": "Variational autoencoders",
    "text": "Variational autoencoders\nhttps://arxiv.org/abs/1312.6114\nhttps://mbernste.github.io/posts/vae/"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#autoencoders-vs-variational-autoencoders",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational autoencoders",
    "section": "Autoencoders vs variational autoencoders",
    "text": "Autoencoders vs variational autoencoders\nhttps://medium.com/data-science/intuitively-understanding-variational-autoencoders-1bfe67eb5daf"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#elbo",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#elbo",
    "title": "Variational autoencoders",
    "section": "ELBO",
    "text": "ELBO\n\nDefinition\nhttps://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html\n\n\nGradient\nhttps://mpatacchiola.github.io/blog/2021/02/08/intro-variational-inference-2.html"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#kullback-leibler-divergence",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#kullback-leibler-divergence",
    "title": "Variational autoencoders",
    "section": "Kullback Leibler divergence",
    "text": "Kullback Leibler divergence\nhttps://kvfrans.com/deriving-the-kl/"
  },
  {
    "objectID": "posts/2025-09-28-variational-autoencoders/index.html#reparameterization-trick",
    "href": "posts/2025-09-28-variational-autoencoders/index.html#reparameterization-trick",
    "title": "Variational autoencoders",
    "section": "Reparameterization trick",
    "text": "Reparameterization trick\nhttps://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html\nhttps://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/"
  },
  {
    "objectID": "posts/2025-09-11-likelihood-loss/index.html",
    "href": "posts/2025-09-11-likelihood-loss/index.html",
    "title": "Maximum Likelihood Estimation and loss functions",
    "section": "",
    "text": "In this blog post, we will introduce the concept of Maximum Likelihood Estimation (MLE) and we will show that in a supervised learning setting, maximizing the likelihood (MLE) of the observed data \\(p(x,\\ y\\ |\\ \\theta)\\) is equivalent to:"
  },
  {
    "objectID": "posts/2025-09-11-likelihood-loss/index.html#general-setting",
    "href": "posts/2025-09-11-likelihood-loss/index.html#general-setting",
    "title": "Maximum Likelihood Estimation and loss functions",
    "section": "General setting",
    "text": "General setting\nThe Maximum Likelihood Estimation (MLE) seeks to find the parameters \\(\\theta\\) of a probabilistic model that maximize the likelihood \\(L\\) of the observed data \\(x\\). If we consider that \\(x\\) is composed of \\(n\\) independent and identically distributed samples \\(x_{i}\\), \\(L\\) can be expressed as:\n\\(L(\\theta) = \\prod_{i = 1}^{n}p(x_{i}\\ |\\ \\theta)\\)\nBy convenience and for numerical stability reasons, the \\(\\log\\) of \\(L(\\theta)\\) is often maximized instead of \\(L(\\theta)\\) directly.\n\\(log(L(\\theta)) = \\sum_{i = 1}^{n}log\\ p(x_{i}\\ |\\ \\theta)\\)"
  },
  {
    "objectID": "posts/2025-09-11-likelihood-loss/index.html#supervised-learning-setting",
    "href": "posts/2025-09-11-likelihood-loss/index.html#supervised-learning-setting",
    "title": "Maximum Likelihood Estimation and loss functions",
    "section": "Supervised learning setting",
    "text": "Supervised learning setting\nIn a supervised learning setting:\n\nThe samples are pairs \\((x_{i},y_{i})\\) of observed sample \\(x_{i}\\) and label \\(y_{i}\\).\n\nThus, the likelihood is expressed as:\n\\(L(\\theta) = \\prod_{i = 1}^{n}p(x_{i},\\ y_{i}\\ |\\ \\theta)\\)\nwhich can be rearranged as\n\\(L(\\theta) = \\prod_{i = 1}^{n}p(y_{i}\\ |\\ x_{i},\\ \\theta)\\ p(x_{i}\\ |\\ \\theta)\\)\n\nMoreover, in a supervised learning setting, the model does not try to model the data \\(x_{i}\\) but only the label \\(y_{i}\\) based on \\(x_{i}\\).\n\nThus, \\(p(x_{i})\\) does not depend on \\(\\theta\\) and maximizing \\(p(y_{i}\\ |\\ x_{i},\\ \\theta)\\ p(x_{i}\\ |\\ \\theta)\\) with respect to \\(\\theta\\) is equivalent to maximizing \\(p(y_{i}\\ |\\ x_{i},\\ \\theta)\\).\n\\(L(\\theta)\\  \\propto \\ \\prod_{i = 1}^{n}p(y_{i}\\ |\\ x_{i},\\ \\theta)\\)\nIn other terms, in a supervised learning setting, maximizing the likelihood \\(p(x,\\ y\\ |\\ \\theta)\\) is equivalent to maximizing \\(p(y\\ |\\ x,\\ \\theta)\\)."
  },
  {
    "objectID": "posts/2025-09-11-likelihood-loss/index.html#classification-setting",
    "href": "posts/2025-09-11-likelihood-loss/index.html#classification-setting",
    "title": "Maximum Likelihood Estimation and loss functions",
    "section": "Classification setting",
    "text": "Classification setting\nIn a classification setting with \\(C\\) classes:\n\nThe label \\(c_{i}\\) is the class index for sample \\(i\\).\n\nWe note \\(y_{i}\\) the corresponding one-hot vector of size \\(C\\) with elements\n\n\\(y_{i,k} = 1\\) for \\(k = c_{i}\\)\n\\(y_{i,k} = 0\\) for \\(k \\neq c_{i}\\)\n\n\nThe model outputs the estimated probabilities of each class in a vector \\(\\widehat{y_{i}} = f(x_{i})\\) of size \\(C\\), i.e.\n\n\\(p(c_{i}\\ |\\ x_{i},\\ \\theta)\\  \\triangleq \\ \\widehat{y_{i,c_{i}}}\\)\nWhich can also be written as\n\\(p(c_{i}\\ |\\ x_{i},\\ \\theta)\\  = \\prod_{k = 1}^{C}\\ {\\widehat{y_{i,k}}}^{{\\ y}_{i,k}}\\)\n\nThus, \\(L(\\theta)\\  \\propto \\ \\prod_{i = 1}^{n}\\prod_{k = 1}^{C}\\ {\\widehat{y_{i,k}}}^{{\\ y}_{i,k}}\\)\nTaking the \\(\\log\\) of the expression above gives\n\\(\\sum_{i = 1}^{n}\\sum_{k = 1}^{C}y_{i,k}\\ log\\ \\widehat{y_{i,k}}\\)\nFinally, maximizing the expression above is equivalent to minimizing its negative, i.e\n\\(- \\sum_{i = 1}^{n}\\sum_{k = 1}^{C}y_{i,k}\\ log\\ \\widehat{y_{i,k}}\\)\nWe can recognize above the expression of the cross entropy loss.\n\nUsing the fact that \\(y_{i,k} = 1\\) if \\(k = c_{i}\\) and \\(y_{i,k} = O\\) for \\(k \\neq c_{i}\\), the expression can be simplified to\n\\(- \\sum_{i = 1}^{n}log\\ \\widehat{y_{i,c_{i}}}\\)\n\nThus, in the supervised classification setting, maximizing the likelihood (MLE) is equivalent to minimizing the cross entropy loss.\nTo understand why the formula \\(- \\sum_{i = 1}^{n}\\sum_{k = 1}^{C}y_{i,k}\\ log\\ \\widehat{y_{i,k}}\\) is called the cross entropy loss, refer to the dedicated section in the Information Theory blog post."
  },
  {
    "objectID": "posts/2025-09-11-likelihood-loss/index.html#regression-setting",
    "href": "posts/2025-09-11-likelihood-loss/index.html#regression-setting",
    "title": "Maximum Likelihood Estimation and loss functions",
    "section": "Regression setting",
    "text": "Regression setting\nIn a regression setting\n\nThe label is \\(y_{i}\\) for sample \\(i\\).\nThe model outputs a prediction \\(\\widehat{y_{i}} = f(x_{i})\\).\n\nIf we assume that the error follows a normal distribution, we have\n\\(y_{i} \\sim N(\\widehat{y_{i}},\\sigma^{2})\\)\nand\n\\(p(y_{i}\\ |\\ x_{i},\\ \\theta)\\  = \\frac{1}{}e^{- \\frac{{(y_{i} - \\widehat{y_{i}})}^{2}}{2\\sigma^{2}}}\\)\n\nThus, \\(L(\\theta)\\  \\propto \\ \\prod_{i = 1}^{n}\\frac{1}{}e^{- \\frac{{(y_{i} - \\widehat{y_{i}})}^{2}}{2\\sigma^{2}}}\\)\nAnd maximizing the expression above is equivalent to minimizing \\({(y_{i} - \\widehat{y_{i}})}^{2}\\)\nTaking the \\(\\log\\) of the expression above gives\n\\(\\sum_{i = 1}^{n}\\log\\frac{1}{} - \\frac{{(y_{i} - \\widehat{y_{i}})}^{2}}{2\\sigma^{2}}\\)\nFinally, because only \\(\\widehat{y_{i}}\\) depends on \\(\\theta\\), maximizing the expression above is equivalent to minimizing\n\\(\\sum_{i = 1}^{n}{(y_{i} - \\widehat{y_{i}})}^{2}\\)\nWe can recognize above the expression of the mean squared error (MSE).\nThus, in the regression setting and under the assumption of normally distributed errors, maximizing the likelihood (MLE) is equivalent to minimizing the mean squared error."
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a univariate scalar function, the derivative is simply defined as the derivative of the function.\n\\(\\frac{df}{d\\theta} \\in R\\)\n\n\n\n\n\n\n\n\nFor a multivariate scalar function, the partial derivatives of \\(f:\\ R^{n} \\rightarrow R\\) can be arranged in its gradient:\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\).\nThe gradient is a column vector where each element is the partial derivative of \\(f\\) with respect to the corresponding input dimension.\n\n\n\n\n\n\n\n\nThe partial derivatives of \\(f:\\ R^{n} \\rightarrow R^{m}\\) can be arranged in its Jacobian matrix \\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\).\n\\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\) contains\n\nIn its rows, \\({\\nabla_{\\theta}}^{T}f_{i} \\in R^{n}\\) the transpose of the gradient of the components \\(f_{i}\\) of \\(f\\).\n\nEach row \\(i\\) contains the derivative of the component \\(f_{i}\\) with respect to all the input dimensions.\n\nIn its columns, \\(\\frac{\\partial f}{\\partial\\theta_{j}}\\) the partial derivatives of \\(f\\) with respect to \\(\\theta_{j}\\).\n\nEach column \\(j\\) contains the derivation of all the components of \\(f\\) with respect to \\(\\theta_{j}\\) (the input dimension \\(j\\)).\n\n\n\\(\\frac{\\partial f}{\\partial\\theta}^{T} = \\left\\lbrack \\nabla_{\\theta}f_{1},\\ \\nabla_{\\theta}f_{2},\\ ...,\\ \\nabla_{\\theta}f_{m} \\right\\rbrack \\in R^{n\\  \\times \\ m}\\)\n\n\n\\(f\\) is a multivariate scalar function and its Jacobian is the transpose of its gradient.\n\n\n\n\\(f\\) is an univariate scalar function."
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a univariate scalar function, the derivative is simply defined as the derivative of the function.\n\\(\\frac{df}{d\\theta} \\in R\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "For a multivariate scalar function, the partial derivatives of \\(f:\\ R^{n} \\rightarrow R\\) can be arranged in its gradient:\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\).\nThe gradient is a column vector where each element is the partial derivative of \\(f\\) with respect to the corresponding input dimension."
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-vector-valued-function",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-vector-valued-function",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "",
    "text": "The partial derivatives of \\(f:\\ R^{n} \\rightarrow R^{m}\\) can be arranged in its Jacobian matrix \\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\).\n\\(\\frac{\\partial f}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\) contains\n\nIn its rows, \\({\\nabla_{\\theta}}^{T}f_{i} \\in R^{n}\\) the transpose of the gradient of the components \\(f_{i}\\) of \\(f\\).\n\nEach row \\(i\\) contains the derivative of the component \\(f_{i}\\) with respect to all the input dimensions.\n\nIn its columns, \\(\\frac{\\partial f}{\\partial\\theta_{j}}\\) the partial derivatives of \\(f\\) with respect to \\(\\theta_{j}\\).\n\nEach column \\(j\\) contains the derivation of all the components of \\(f\\) with respect to \\(\\theta_{j}\\) (the input dimension \\(j\\)).\n\n\n\\(\\frac{\\partial f}{\\partial\\theta}^{T} = \\left\\lbrack \\nabla_{\\theta}f_{1},\\ \\nabla_{\\theta}f_{2},\\ ...,\\ \\nabla_{\\theta}f_{m} \\right\\rbrack \\in R^{n\\  \\times \\ m}\\)\n\n\n\\(f\\) is a multivariate scalar function and its Jacobian is the transpose of its gradient.\n\n\n\n\\(f\\) is an univariate scalar function."
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function-1",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#univariate-scalar-function-1",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Univariate scalar function",
    "text": "Univariate scalar function\nWhen univariate scalar functions are composed, the derivative is the product of the intermediate derivatives.\n\n\n\n\n\n\\(\\frac{df}{d\\theta} = \\frac{df}{dz}\\frac{dz}{d\\theta}\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function-1",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#multivariate-scalar-function-1",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Multivariate scalar function",
    "text": "Multivariate scalar function\n\n\n\n\n\n\\(f:\\ R^{n} \\rightarrow R\\) is a multivariate scalar function. Its partial derivatives can be arranged in its gradient.\n\\({\\nabla_{\\theta}}^{T}f = \\left\\lbrack \\frac{\\partial f}{\\partial\\theta_{1}},\\ \\frac{\\partial f}{\\partial\\theta_{2}},\\ ...,\\ \\frac{\\partial f}{\\partial\\theta_{n}} \\right\\rbrack \\in R^{n}\\)\nFor each input dimension \\(\\theta_{i}\\), the chain rule is applied to each intermediate dimension \\(z_{j}\\) and the derivative is the sum over \\(j\\).\nThus,\n\\(\\frac{\\partial f}{\\partial\\theta_{i}} = \\sum_{j = 1}^{m}\\frac{\\partial f}{\\partial z_{j}}\\frac{\\partial z_{j}}{\\partial\\theta_{i}}\\)\nThe expression of \\({\\nabla_{\\theta}}^{T}f\\) above can also be expressed with:\n\nThe Jacobian of \\(z\\) with respect to \\(\\theta\\): \\(\\frac{\\partial z}{\\partial\\theta} \\in R^{m\\  \\times \\ n}\\)\nThe gradient of \\(f\\) with respect to \\(z\\): \\(\\nabla_{z}f \\in R^{m}\\)\n\n\\(\\nabla_{\\theta}f = \\left( \\frac{\\partial z}{\\partial\\theta} \\right)^{T}\\nabla_{z}f\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-al",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-al",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Derivative with respect to \\(a^{l}\\)",
    "text": "Derivative with respect to \\(a^{l}\\)\nTo compute the derivative of \\(E\\) with respect to \\(a^{l}\\), the activation of layer \\(l\\), we express \\(E\\) as the composition of \\(E(z^{l + 1})\\) and \\(z^{l + 1}(a^{l},W^{l + 1})\\):\n\\(E(\\widehat{y},\\ y) = E(...(z^{l + 1}(a^{l},W^{l + 1})))\\)\n\\(E(a^{l})\\) is thus a multivariate scalar function with intermediate variables \\(z^{l + 1}\\).\n\n\n\n\n\nAccording to the chain rule for a multivariate scalar function, its derivative is given by its gradient:\n\\(\\nabla_{a^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial a_{i}^{l}},\\ ... \\right\\rbrack\\)\nwhere \\(\\frac{\\partial E}{\\partial a_{i}^{l}} = \\frac{\\partial E}{\\partial z^{l + 1}}\\frac{\\partial z^{l + 1}}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}\\frac{\\partial z_{k}^{l + 1}}{\\partial a_{i}^{l}}\\) Multivariate chain rule\nMoreover, \\(z_{j}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ij}^{l}a_{i}^{l - 1}\\), thus \\(\\frac{\\partial z_{j}^{l}}{\\partial a_{i}^{l - 1}} = w_{ij}^{l}\\) and the expression above becomes:\n\\(\\frac{\\partial E}{\\partial a_{i}^{l}} = \\sum_{k = 1}^{N^{l + 1}}\\frac{\\partial E}{\\partial z_{k}^{l + 1}}{\\ w}_{ik}^{l + 1}\\)"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-zl",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-zl",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Derivative with respect to \\(z^{l}\\)",
    "text": "Derivative with respect to \\(z^{l}\\)\nSimilarly, we can express \\(E\\) as the composition of \\(E(a^{l})\\) and \\(a^{l}(z^{l})\\):\n\\(E(\\widehat{y},\\ y) = E(...z^{l + 1}(a^{l}(z^{l}),W^{l + 1})))\\)\n\\(E(z^{l})\\) is thus a multivariate scalar function and its derivative is given by its gradient:\n\\(\\nabla_{z^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial z_{i}^{l}},\\ ... \\right\\rbrack\\)\nNotice also that each function \\(E(z_{i}^{l})\\) can be expressed as the composition of \\(E(a_{i}^{l})\\) and \\(a_{i}^{l}(z_{i}^{l})\\).\nThus, the chain rule for univariate scalar functions can be applied:\n\\(\\frac{\\partial E}{\\partial z_{i}^{l}} = \\frac{\\partial E}{\\partial a_{i}^{l}}\\frac{\\partial a_{i}^{l}}{\\partial z_{i}^{l}}\\) Scalar chain rule"
  },
  {
    "objectID": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-w_ijl",
    "href": "posts/2025-09-02-derivatives-and-backpropagation/index.html#derivative-with-respect-to-w_ijl",
    "title": "Derivatives, chain rule and backpropagation",
    "section": "Derivative with respect to \\(w_{ij}^{l}\\)",
    "text": "Derivative with respect to \\(w_{ij}^{l}\\)\nFinally we can express \\(E\\) as the composition of \\(E(z^{l})\\) and \\(z^{l}(a^{l - 1},W^{l})\\):\n\\(E(\\widehat{y},\\ y) = E(...z^{l}(a^{l - 1},W^{l})))\\)\n\\(E(W^{l})\\) is thus a multivariate scalar function with intermediate variables \\(z^{l}\\).\n\n\n\n\n\nAccording to the chain rule for a multivariate scalar function, its derivative is given by its gradient:\n\\(\\nabla_{W^{l}}\\ E = \\left\\lbrack \\frac{\\partial E}{\\partial w_{ij}^{l}},\\ ... \\right\\rbrack\\)\nwhere \\(\\frac{\\partial E}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z^{l}}\\frac{\\partial z^{l}}{\\partial w_{ij}^{l}} = \\sum_{k = 1}^{N^{l}}\\frac{\\partial E}{\\partial z_{k}^{l}}\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w_{ij}^{l}}\\) Multivariate chain rule\n\n\n\n\n\nMoreover, \\(z_{k}^{l} = \\sum_{i = 1}^{N^{l - 1}}w_{ik}^{l}a_{i}^{l - 1}\\)\nThus \\(\\frac{\\partial z_{k}^{l}}{\\partial w_{ij}^{l}} = 0\\) for \\(k \\neq j\\)\nAnd the expression above becomes:\n\\(\\frac{\\partial E}{\\partial w_{ij}^{l}} = \\frac{\\partial E}{\\partial z_{j}^{l}}\\frac{\\partial z_{j}^{l}}{\\partial w_{ij}^{l}}\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Expectation and derivative\n\n\n\n\n\n\nProbabilites\n\nCalculus\n\n\n\nExplore how to handle derivative when expectation is involved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariational autoencoders\n\n\n\n\n\n\nMachine Learning\n\n\n\nA deep dive into variational autoencoders.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformation theory\n\n\n\n\n\n\nInformation theory\n\n\n\nIntroduction to information theory, entropy, cross-entropy and Kullback–Leibler divergence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation and loss functions\n\n\n\n\n\n\nStatistics\n\n\n\nIntroduction to Maximum Likelihood Estimation (MLE) and its relationship with the cross-entropy and mean squared error (MSE) loss functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivatives, chain rule and backpropagation\n\n\n\n\n\n\nCalculus\n\n\n\nThis post is a brief summary of how derivatives and chain rule are defined for scalar functions and vector-valued functions with univariate or multivariate inputs and how they are applied to the backpropagation algorithm.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html",
    "href": "posts/2025-09-11-information-theory/index.html",
    "title": "Information theory",
    "section": "",
    "text": "This document is an introduction to information theory and concepts such as:\nIt also explains how these concepts are related to Maximum Likelihood Estimation."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#introduction",
    "href": "posts/2025-09-11-information-theory/index.html#introduction",
    "title": "Information theory",
    "section": "Introduction",
    "text": "Introduction\nIn information theory, the information content quantifies the level of surprise of an event.\nIt is derived from the probability of occurrence of a particular event.\nIts different names are:\n\nInformation content\n\n\n\nSelf-information\nSurprisal\nShannon information\n\nIt can be thought of as an alternative way of expressing probability."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#origin",
    "href": "posts/2025-09-11-information-theory/index.html#origin",
    "title": "Information theory",
    "section": "Origin",
    "text": "Origin\n\nThe self-information of an event \\(x\\) is noted \\(I(x)\\).\n\nIt is a function \\(f\\) of the probability \\(p\\) that \\(x\\) occurs.\ni.e. \\(I(x) = f(p(x))\\)\n\n\n\n\nClaude Shannon's definition of self-information was chosen to meet several axioms\n\n\\(f(p)\\) is monotonically decreasing in \\(p\\).\n\nThe more probable an event is, the less surprising it is and the less information it yields.\n\n\\(f(1) = 0\\)\n\nAn event with probability 1 is perfectly unsurprising and yields no information.\n\n\\(f(p_{1}\\ p_{2}) = f(p_{1}) + f(p_{2})\\)\n\nThe information from independent events is the sum of the information of each event."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#definition",
    "href": "posts/2025-09-11-information-theory/index.html#definition",
    "title": "Information theory",
    "section": "Definition",
    "text": "Definition\n\nThere is a unique function \\(f\\) that meets the 3 axioms, up to a scaling factor \\(b\\).\n\\(I(x) = - \\log_{b}(p(x))\\)\nDifferent choices of \\(b\\) correspond to different units of information:\n\n\\(b = 2\\): Shannon or bit.\n\\(b = e\\): natural unit of information."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#definition-1",
    "href": "posts/2025-09-11-information-theory/index.html#definition-1",
    "title": "Information theory",
    "section": "Definition",
    "text": "Definition\n\nNoted \\(H(X)\\).\nExpected value of the self-information of a random variable.\nExpected value of \\(I(X)\\):\n\\(H(X)\\)\n\\(= E(I(X))\\)\n\\(= \\ E( - log(p(X)))\\)\n\\(= \\ \\sum_{\\chi}^{} - p(x)\\ log(p(x))\\)"
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#interpretations",
    "href": "posts/2025-09-11-information-theory/index.html#interpretations",
    "title": "Information theory",
    "section": "Interpretations",
    "text": "Interpretations\n\nThe entropy \\(H_{b}(X)\\) in base \\(b\\) is the lower bound of the expected length of any code with an alphabet size of \\(b\\) that encodes events of \\(X\\).\n\nThe length is the number of characters, i.e. the number of units of information.\nThus, its unit is the unit set by the choice of \\(b\\):\n\n\\(b = 2\\): number of bits.\n\\(b = 10\\): number of decimal digits.\n\nEach character can take a value in an alphabet of size \\(b\\).\nSee relationship to encoding for more details.\n\nThis can also be viewed as the expected amount of information needed to describe the state of the variable, considering the distribution of probabilities across all potential states.\n\n\nRelationship to encoding\n\nProved by Claude Shannon.\nFor optimal encoding, the number of characters (bits for \\(b = 2\\)) used per symbol should reflect its information content.\n\nMore probable symbols require fewer bits, while less probable symbols require more bits.\nThis forms the basis of variable-length codes like Huffman coding or arithmetic coding."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#properties",
    "href": "posts/2025-09-11-information-theory/index.html#properties",
    "title": "Information theory",
    "section": "Properties",
    "text": "Properties\n\nThe entropy is maximized when the probability distribution is uniform.\n\nThe uncertainty (i.e. surprise or information content) is maximized as all events are equally probable."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#history",
    "href": "posts/2025-09-11-information-theory/index.html#history",
    "title": "Information theory",
    "section": "History",
    "text": "History\n\nDiscovered by Claude Shannon.\nNamed after the thermodynamics entropy definition by Boltzmann (and generalized by Gibbs).\nVon Neumann realized first that relation."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#forward-kullback-leibler-divergence",
    "href": "posts/2025-09-11-information-theory/index.html#forward-kullback-leibler-divergence",
    "title": "Information theory",
    "section": "Forward Kullback-Leibler divergence",
    "text": "Forward Kullback-Leibler divergence\n\n\\(D_{KL}(p\\ ||\\ q) = \\sum_{\\chi}^{}p(x)\\ log(\\frac{p(x)}{q(x)})\\)\nThe sum or expectation is weighted by the distribution \\(p\\).\nDifferences between \\(p\\) and \\(q\\) produce large \\(D_{KL}\\) on the mass of \\(p\\) (where \\(p &gt; 0\\)) but not where \\(p\\) is small (\\(p \\simeq 0\\)).\nMinimizing the forward Kullback-Leibler divergence \\(D_{KL}(p\\ ||\\ q)\\) encourages \\(q\\) to put its mass wherever the mass of \\(p\\) is (putting its mass outside the mass of \\(p\\) is not penalized).\nThis results in mode covering behavior."
  },
  {
    "objectID": "posts/2025-09-11-information-theory/index.html#reversed-kullback-leibler-divergence",
    "href": "posts/2025-09-11-information-theory/index.html#reversed-kullback-leibler-divergence",
    "title": "Information theory",
    "section": "Reversed Kullback-Leibler divergence",
    "text": "Reversed Kullback-Leibler divergence\n\n\\(D_{KL}(q\\ ||\\ p) = \\sum_{\\chi}^{}q(x)\\ log(\\frac{q(x)}{p(x)})\\)\nThe sum or expectation is weighted by the distribution \\(q\\).\nDifferences between \\(p\\) and \\(q\\) produce large \\(D_{KL}\\) on the mass of \\(q\\) (where \\(q &gt; 0\\)) but not where \\(q\\) is small (\\(q \\simeq 0\\)).\nMinimizing the reversed Kullback-Leibler divergence \\(D_{KL}(q\\ ||\\ p)\\) encourages \\(q\\) not to put its mass outside the mass of \\(p\\) (missing parts of the mass \\(p\\) is not penalized).\nThis results in mode seeking behavior."
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html",
    "title": "Expectation and derivative",
    "section": "",
    "text": "In this document, we explore how to compute the gradient of an expectation or in other terms, the gradient of a stochastic process.\nIn the most general form, let’s define:\n\n\\(x\\) in a random variable\nwhich follows a distribution \\(p_{\\theta}(x)\\) parametrized by \\(\\theta\\)\n\\(f_{\\theta}\\) is a function of \\(x\\) and is also parametrized by \\(\\theta\\)\n\nThe question is then how to compute\n\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f_{\\theta}(x)\\rbrack\\)"
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#fixed-distribution-proof",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#fixed-distribution-proof",
    "title": "Expectation and derivative",
    "section": "Fixed distribution proof",
    "text": "Fixed distribution proof\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f_{\\theta}(x)\\rbrack\\)\n\\(= \\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f_{\\theta}(x)\\rbrack\\)\n\\(= \\nabla_{\\theta}\\int_{}^{}p(x)\\ f_{\\theta}(x)\\ dx\\)\n\\(= \\int_{}^{}\\nabla_{\\theta}\\ \\lbrack p(x){\\ f}_{\\theta}(x)\\rbrack\\ dx\\)\n\n\n\\(= \\int_{}^{}p(x)\\ \\nabla_{\\theta}\\ f_{\\theta}(x)\\ dx\\)\n\n\n(because \\(p(x)\\) does not depend on \\(\\theta\\))\n\n\n\\(= E_{p(x)}\\lbrack\\nabla_{\\theta}\\ f_{\\theta}(x)\\rbrack\\)"
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#introduction-1",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#introduction-1",
    "title": "Expectation and derivative",
    "section": "Introduction",
    "text": "Introduction\nIn the general setting where the probability distribution depends on the parameters \\(\\theta\\) of the gradient, the general form is\n\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f_{\\theta}(x)\\rbrack = E_{p_{\\theta}(x)}\\lbrack\\nabla_{\\theta}\\ log\\ p_{\\theta}(x){\\ f}_{\\theta}(x)\\rbrack + E_{p_{\\theta}(x)}\\lbrack\\nabla_{\\theta}\\ f_{\\theta}(x)\\rbrack\\)\nsee Distribution dependent on \\(\\theta\\) proof below.\n\nNotice that if the distribution is fixed, i.e. \\(p_{\\theta}\\) does not depend on \\(\\theta\\) (\\(p_{\\theta} = p\\)), the first term disappears and we find back the formula from the previous Fixed distribution section.\nThe general form above can also be estimated with Monte Carlo.\n\nDistribution dependent on \\(\\theta\\) proof\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f_{\\theta}(x)\\rbrack\\)\n\\(= \\nabla_{\\theta}\\int_{}^{}p_{\\theta}(x)\\ f_{\\theta}(x)\\ dx\\)\n\\(= \\int_{}^{}\\nabla_{\\theta}\\ \\lbrack p_{\\theta}(x){\\ f}_{\\theta}(x)\\rbrack\\ dx\\)\n\\(= \\int_{}^{}\\nabla_{\\theta}\\ p_{\\theta}(x){\\ f}_{\\theta}(x)\\ dx + \\int_{}^{}p_{\\theta}(x)\\ \\nabla_{\\theta}\\ f_{\\theta}(x)\\ dx\\)\n\n\n\\(= \\int_{}^{}p_{\\theta}(x)\\ \\nabla_{\\theta}\\ log\\ p_{\\theta}(x){\\ f}_{\\theta}(x)\\ dx + E_{p_{\\theta}(x)}\\lbrack\\nabla_{\\theta}\\ f_{\\theta}(x)\\rbrack\\)\n\n\n(see Log derivative trick)\n\n\n\\(= E_{p_{\\theta}(x)}\\lbrack\\nabla_{\\theta}\\ log\\ p_{\\theta}(x){\\ f}_{\\theta}(x)\\rbrack + E_{p_{\\theta}(x)}\\lbrack\\nabla_{\\theta}\\ f_{\\theta}(x)\\rbrack\\)\n\n\nLog derivative trick\n\nThe log derivative trick above is the fact that \\(\\nabla_{\\theta}\\ p_{\\theta}(x) = p_{\\theta}(x)\\ \\nabla_{\\theta}\\ log\\ p_{\\theta}(x)\\).\nThis comes from the application of the chain rule to the \\(\\log\\) function:\n\n\\(\\nabla_{x}\\ log(f(x))\\)\n\n\\(= \\nabla_{f}\\ log(f)\\ \\nabla_{x}\\ f(x)\\)\n\\(= \\frac{1}{f(x)}\\ \\nabla_{x}\\ f(x)\\)\nThus, \\(\\nabla_{x}\\ f(x) = f(x){\\ \\nabla}_{x}\\ log(f(x))\\)\n\nFor the following, let us simplify our problem by making the function \\(f_{\\theta}\\) independent on \\(\\theta\\) (i.e. \\(f_{\\theta} = f\\)). This independence is verified in common use cases such as variational autoencoders or reinforcement learning.\n\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack = E_{p_{\\theta}(x)}\\lbrack{f(x)\\ \\nabla}_{\\theta}\\ log\\ p_{\\theta}(x)\\rbrack\\)\n\nThe term above (or the first term in the general form if we keep \\(f_{\\theta}\\) dependent on \\(\\theta\\)) can be estimated with Monte Carlo but it has high variance (see Score function estimator vs Reparameterization trick).\nThis leaves us with two choices in order to compute the gradient of an expectation when the probability distribution depends on the parameters \\(\\theta\\) of the gradient:\n\nUse this Monte Carlo estimator anyway. This is known as the Score function estimator.\nUse another technique. One of them is called the Reparameterization trick.\n\nWe explore both techniques in the Score function estimator and Reparameterization trick sections below."
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator",
    "title": "Expectation and derivative",
    "section": "Score function estimator",
    "text": "Score function estimator\nIn this technique, the Monte Carlo estimator from the previous section is used directly.\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}{f(x)\\ \\nabla}_{\\theta}\\ log\\ p_{\\theta}(x)\\rbrack\\)\nwith \\(x \\sim p_{\\theta}(x)\\)\n\nThis estimator is called (see Glossary)\n\nThe score function estimator\nThe likelihood ratio\nThe REINFORCE estimator\n\nThe name comes from the fact that in statistics, \\(\\nabla_{\\theta}\\ log\\ p_{\\theta}(x)\\) is called the score or the score function (see Glossary).\nThis is the technique used in reinforcement learning and in the REINFORCE algorithm.\nIt is obtained using the log derivative trick (see Distribution dependent on \\(\\theta\\) proof)."
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#reparameterization-trick",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#reparameterization-trick",
    "title": "Expectation and derivative",
    "section": "Reparameterization trick",
    "text": "Reparameterization trick\n\nAlso called the pathwise estimator.\n\nAs a reminder, the goal is to compute \\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack\\) but because the probability distribution depends on \\(\\theta\\), we cannot directly swap \\(\\nabla_{\\theta}\\) and \\(E_{p_{\\theta}(x)}\\).\nIn the reparametrization trick, the Law of the Unconscious Statistician (LOTUS) is used to reformulate the problem in such a way that the gradient parameters \\(\\theta\\) are moved out of the expectation.\nWe can\n\nExpress our random variable \\(x\\)\n\n\\(x \\sim p_{\\theta}(x)\\)\n\nAs a function \\(g_{\\theta}\\) of another random variable \\(\\varepsilon\\)\n\n\\(x = g_{\\theta}(\\varepsilon)\\) with \\(\\varepsilon \\sim q(\\varepsilon)\\)\n\n\nThen the Law of the Unconscious Statistician (LOTUS) tells us that\n\nThe expectation of \\(x\\) can be computed with the distribution of \\(\\varepsilon\\)\n\n\\(E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack = E_{q(\\varepsilon)}\\lbrack f(g_{\\theta}(\\varepsilon))\\rbrack\\)\n\n\nWith this new formulation, the distribution no longer depends on \\(\\theta\\). Thus we are again in a Fixed distribution setting.\nThus,\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack\\)\n\n\n\\(= \\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f{(\\ g}_{\\theta}(\\varepsilon))\\rbrack\\)\n\n\n(LOTUS)\n\n\n\n\n\\(= E_{q(\\varepsilon)}\\lbrack\\nabla_{\\theta}(f{(\\ g}_{\\theta}(\\varepsilon)))\\rbrack\\)\n\n\n(\\(q\\) does not depend on \\(\\theta\\))\n\n\nwhich can be estimated with Monte Carlo\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack\\)\n\\(\\simeq \\frac{1}{N}\\sum_{i = 1}^{N}\\nabla_{\\theta}(f{(\\ g}_{\\theta}(\\varepsilon_{i})))\\) with \\(\\varepsilon_{i} \\sim q(\\varepsilon)\\)"
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator-vs-reparameterization-trick",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator-vs-reparameterization-trick",
    "title": "Expectation and derivative",
    "section": "Score function estimator vs Reparameterization trick",
    "text": "Score function estimator vs Reparameterization trick\n\n\n\n\n\n\n\nScore function estimator\nReparameterization trick\n\n\n\n\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack = E_{p_{\\theta}(x)}\\lbrack f(x)\\ \\nabla_{\\theta}\\ log\\ p_{\\theta}(x)\\rbrack\\)\n\\(\\nabla_{\\theta}E_{p_{\\theta}(x)}\\lbrack f(x)\\rbrack = E_{q(\\varepsilon)}\\lbrack\\nabla_{\\theta}(f{(\\ g}_{\\theta}(\\varepsilon)))\\rbrack\\)\n\n\nCan work with a non-differentiable model.\nThus, often used in reinforcement learning where the environment dynamics are usually non-differentiable.\nRequires \\(f\\) and \\(g_{\\theta}\\) to be differentiable.\nThus, requires a differentiable model.\n\n\nGradient has high variance because the gradient term \\(\\nabla_{\\theta}\\) is multiplied by an unrelated quantity \\(f(x)\\).\n- \\(f(x)\\) might be large, noisy or high-variance.\n- the gradient term \\(\\nabla_{\\theta}\\) gets scaled unpredictably by \\(f(x)\\).\nGradient has low variance because it only contains gradient terms \\(\\nabla_{\\theta}\\).\n\n\n\nBelow is a graphical comparison of the score function estimator vs the reparameterization trick.\n\n\n\n\n\nWe can see that the gradient obtained with reparameterization trick is direct while the one obtained with the score function estimator contains one part with \\(\\nabla_{\\theta}\\) but can be perturbed by the other part \\(f(x)\\)."
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#score-score-function",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#score-score-function",
    "title": "Expectation and derivative",
    "section": "Score / Score function",
    "text": "Score / Score function\nGradient of the log-likelihood \\(= \\nabla_{\\theta}\\ log\\ p_{\\theta}(x)\\)"
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator-likelihood-ratio-reinforce-estimator",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-estimator-likelihood-ratio-reinforce-estimator",
    "title": "Expectation and derivative",
    "section": "Score function estimator / Likelihood ratio / REINFORCE estimator",
    "text": "Score function estimator / Likelihood ratio / REINFORCE estimator\nMonte Carlo estimator of the gradient of an expectation (when the distribution depends on the gradient parameters).\n\\({\\nabla_{\\theta}E}_{p(x)}\\lbrack f(x)\\rbrack \\simeq \\frac{1}{N}\\sum_{i = 1}^{N}{f(x)\\ \\nabla}_{\\theta}\\ log\\ p_{\\theta}(x)\\rbrack\\) with \\(x \\sim p_{\\theta}(x)\\)\nThe name comes from the fact that the score function \\(\\nabla_{\\theta}\\ log\\ p_{\\theta}(x)\\) appears in the formula."
  },
  {
    "objectID": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-vs-reparametrization-trick",
    "href": "posts/2025-09-28-expectation-and-derivative/index.html#score-function-vs-reparametrization-trick",
    "title": "Expectation and derivative",
    "section": "Score function vs reparametrization trick",
    "text": "Score function vs reparametrization trick\nhttps://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/\nhttps://mpatacchiola.github.io/blog/2021/02/08/intro-variational-inference-2.html\nhttps://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html"
  }
]